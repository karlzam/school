{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project - Orca Call Clustering \n",
    "Karlee Zammit - V00823093 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/earthspecies/library/tree/main/orcas\n",
    "\n",
    "https://debuggercafe.com/pytorch-pretrained-efficientnet-model-image-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kzammit\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\kzammit\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torchaudio\\backend\\utils.py:89: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io, transform\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#import umap.plot\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#from sklearn.datasets import load_digits\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#from sklearn.model_selection import train_test_split\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#from sklearn.preprocessing import MinMaxScaler\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#from sklearn.cluster import KMeans\u001b[39;00m\n\u001b[0;32m     30\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install torch torchvision efficientnet_pytorch==0.7.0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\umap\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\umap\\umap_.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tril \u001b[38;5;28;01mas\u001b[39;00m sparse_tril, triu \u001b[38;5;28;01mas\u001b[39;00m sparse_triu\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistances\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msparse\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import os.path, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "import librosa\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "from skimage import io, transform\n",
    "from PIL import Image, ImageOps\n",
    "import umap\n",
    "#import umap.plot\n",
    "#from sklearn.datasets import load_digits\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.cluster import KMeans\n",
    "!pip install torch torchvision efficientnet_pytorch==0.7.0\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save the waveform from the wave file \n",
    "def plot_waveform(idx, waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "    axes.grid(True)\n",
    "    figure.suptitle(title)\n",
    "    #plt.show(block=False)\n",
    "    #plt.savefig(r'C:\\Users\\kzammit\\Repos\\school\\phys555\\project-DL\\orcas_classification\\wavpngs\\wav-' + str(idx) + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save the spectrogram from the wave file\n",
    "#def create_spectrogram(idx, specgram, title=None, ylabel=\"freq_bin\"):\n",
    "def create_spectrogram(idx, specgram):\n",
    "    #fig, axs = plt.subplots(1, 1)\n",
    "    #axs.set_title(title or \"Spectrogram (db)\")\n",
    "    #axs.set_ylabel(ylabel)\n",
    "    #axs.set_xlabel(\"frame\")\n",
    "    #axs.set_ylim(0, 100)\n",
    "    #print(specgram.shape)\n",
    "    #imgf = np.flip(specgram, axis=0)\n",
    "    transform=transforms.ToPILImage()\n",
    "    #specgram=transform(specgram)\n",
    "    specgram= transform(specgram)\n",
    "    specgram = ImageOps.flip(specgram)\n",
    "    specgram.save(r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\specs-db\\spec-\" + str(idx) + \".png\")\n",
    "    #display(specgram)\n",
    "    \n",
    "    #im = Image.fromarray(librosa.power_to_db(specgram))\n",
    "    #if im.mode != 'RGB':\n",
    "    #    im = im.convert('RGB')\n",
    "    #im.save(\"your_file.jpeg\")\n",
    "    #im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    # librosa... is a numpy array, save that to a png file instead \n",
    "    #fig.colorbar(im, ax=axs)\n",
    "    #plt.show(block=False)\n",
    "    #plt.savefig(r'C:\\Users\\kzammit\\Repos\\school\\phys555\\project-DL\\orcas_classification\\spectrograms\\spec-' + str(idx) + '.png')\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the spectrogram figures and save as pngs\n",
    "def crop(path, dirs):\n",
    "    for item in dirs:\n",
    "        fullpath = os.path.join(path,item)\n",
    "        if os.path.isfile(fullpath):\n",
    "            im = Image.open(fullpath)\n",
    "            f, e = os.path.splitext(fullpath)\n",
    "            # last one does the height\n",
    "            # left top right bottom\n",
    "            imCrop = im.crop((80, 58, 475, 425)) #corrected\n",
    "            #imCrop.save(f + '-crop.png', \"PNG\", quality=300)\n",
    "            display(imCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    transform =  transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()])\n",
    "        \n",
    "        #transforms.Normalize(mean=[0.13214], \n",
    "        #                     std=[0.04182]),]) # this is not the way to go \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spectrogram Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/main/generated/torchaudio.transforms.AmplitudeToDB.html\n",
    "# For each available wave file\n",
    "for ii in range(0, 594):\n",
    "    \n",
    "    # In the audio folder \n",
    "    path = r'C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\audio'\n",
    "    \n",
    "    # Get the file name and path\n",
    "    file = '\\\\' + str(ii) + '.wav'\n",
    "    fullfile = path + file\n",
    "\n",
    "    # Set the sample to this file\n",
    "    SAMPLE_WAV = fullfile\n",
    "\n",
    "    # Load the wave with torchaudio \n",
    "    ORCA_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_WAV, normalize=True)\n",
    "    transform = T.AmplitudeToDB(stype=\"amplitude\", top_db=100)\n",
    "    orca_waveform_db = transform(ORCA_WAVEFORM)\n",
    "\n",
    "    # Plot the original waveform \n",
    "    #plot_waveform(idx=str(ii), waveform=ORCA_WAVEFORM, sr=SAMPLE_RATE, title=\"Original waveform\")\n",
    "    #plot_waveform(idx=str(ii), waveform=orca_waveform_db, sr=SAMPLE_RATE, title=\"Original waveform\")\n",
    "    #Audio(ORCA_WAVEFORM.numpy(), rate=SAMPLE_RATE)\n",
    "    Audio(orca_waveform_db.numpy(), rate=SAMPLE_RATE)\n",
    "\n",
    "    # Set the spectrogram parameters \n",
    "    n_fft = 2000\n",
    "    win_length = 2000\n",
    "    hop_length = 100\n",
    "\n",
    "    # Define spectrogram settings using torch audio transforms  \n",
    "    spectrogram = T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length, center=True, pad_mode=\"reflect\", power=2.0)\n",
    "    \n",
    "    # Calculate the spectrogram \n",
    "    #spec = spectrogram(ORCA_WAVEFORM)\n",
    "    spec = spectrogram(orca_waveform_db)\n",
    "    \n",
    "    # Plot and save the spectrogram\n",
    "    create_spectrogram(idx=str(ii), specgram=spec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in ORCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrcaImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, annotations_file, img_dir, transform, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        image = image.squeeze()\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\annotations-edited.csv\"\n",
    "#img_dir = r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\specs\"\n",
    "img_dir = r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\specs-db\"\n",
    "transforms_eff = preprocess()\n",
    "orca_dataset = OrcaImageDataset(annotations_file, img_dir, transform=transforms_eff)\n",
    "labels = orca_dataset.img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xtrain_data, test_data, ytrain_labels, test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43morca_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#  split a validation set from the training set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_data, val_data, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m      6\u001b[0m     xtrain_data, ytrain_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22\u001b[39m)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2585\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m-> 2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2587\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2587\u001b[0m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\sklearn\\utils\\__init__.py:358\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_list_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\sklearn\\utils\\__init__.py:212\u001b[0m, in \u001b[0;36m_list_indexing\u001b[1;34m(X, key, key_dtype)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compress(X, key))\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# key is a integer array-like of key\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [X[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m key]\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\sklearn\\utils\\__init__.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compress(X, key))\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# key is a integer array-like of key\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m key]\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mOrcaImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     13\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 14\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     16\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torchvision\\io\\image.py:251\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03mReads a JPEG or PNG image into a 3 dimensional RGB Tensor.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mOptionally converts the image to the desired format.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    output (Tensor[image_channels, image_height, image_width])\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torchvision\\io\\image.py:230\u001b[0m, in \u001b[0;36mdecode_image\u001b[1;34m(input, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_image\u001b[39m(\u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, mode: ImageReadMode \u001b[38;5;241m=\u001b[39m ImageReadMode\u001b[38;5;241m.\u001b[39mUNCHANGED) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    Detects whether an image is a JPEG or PNG and performs the appropriate\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    operation to decode the image into a 3 dimensional RGB Tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m        output (Tensor[image_channels, image_height, image_width])\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Internal error."
     ]
    }
   ],
   "source": [
    "xtrain_data, test_data, ytrain_labels, test_labels = train_test_split(\n",
    "    orca_dataset, labels, test_size=0.2, random_state=21)\n",
    "\n",
    "#  split a validation set from the training set\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    xtrain_data, ytrain_labels, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler()\n",
    "#train_data_sc = scaler.fit_transform(train_data)\n",
    "#df = pd.DataFrame(train_data)\n",
    "#print(df[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of samples for training is ' + str(len(train_data)))\n",
    "print('The number of samples for validation is ' + str(len(val_data)))\n",
    "print('The number of samples for testing is ' + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "for idex, data in enumerate(train_loader): \n",
    "    img, label = train_loader.dataset.__getitem__(idex)\n",
    "    #display(img)\n",
    "    mean = torch.mean(img)\n",
    "    mean = float(mean)\n",
    "    std = torch.std(img)\n",
    "    means.append(mean)\n",
    "    stds.append(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(means)\n",
    "meanf = np.mean(means)\n",
    "print(meanf) \n",
    "\n",
    "stdf = np.std(means)\n",
    "print(stdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the spectrograms and save as images \n",
    "path = r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\example_crop\"\n",
    "dirs = os.listdir(path)\n",
    "crop(path, dirs)\n",
    "\n",
    "# the initial size of the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orca_dataset.img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of tensor for 60th image in train dataset: ', orca_dataset[60][0].shape)\n",
    "print(orca_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orca_dataset[60][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToPILImage()\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    #print(image.shape)\n",
    "    image = transform(image)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off');\n",
    "    #print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_loader, val_loader, n_epochs, optimizer):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_batch_losses = []\n",
    "        val_batch_losses = []\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            #img = img.squeeze()\n",
    "            #print(img.shape)\n",
    "            img = img.to(device)\n",
    "            #img = img.reshape(-1, 28*28)\n",
    "            \n",
    "            #print(img.shape)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, img.data)\n",
    "            #************************ backward *************************\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_batch_losses.append(loss.item()) \n",
    "            \n",
    "        model = model.eval()\n",
    "        for data in val_loader:\n",
    "            output = model(img)\n",
    "            loss = criterion(output, img.data)\n",
    "            val_batch_losses.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(train_batch_losses)\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "            \n",
    "        # ***************************** log ***************************\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"epoch [{epoch + 1}/{n_epochs}], Train loss:{train_loss: .4f} Valid:{val_loss: .4f}\")\n",
    "            \n",
    "    ax = plt.figure().gca()\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'])\n",
    "    plt.title('Loss monitoring')\n",
    "    plt.show()\n",
    "    \n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DeepAutoencoder class\n",
    "class DeepAutoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            #torch.nn.Linear(28 * 28, 256),\n",
    "            torch.nn.Linear(224, 224),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(224, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 10)\n",
    "        )\n",
    "          \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 224),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(224, 224),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model and hyperparameters\n",
    "model = DeepAutoencoder().to(device)\n",
    "print(model)\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "linear_model = train_test_model(model, train_loader, val_loader, n_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## encoder layers ##\n",
    "        \n",
    "        self.encoder = nn.Sequential( \n",
    "            nn.Conv2d(1,16,3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(16,4,3, padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4,16,2, stride=2), \n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose2d(16,1,2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the NN\n",
    "model = ConvAutoencoder().to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "n_epochs=100\n",
    "cnn_model = train_test_model(model, train_loader, val_loader, n_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToPILImage()\n",
    "fig1 = plt.figure(figsize=(16, 6))\n",
    "fig2 = plt.figure(figsize=(16, 6))\n",
    "fig3 = plt.figure(figsize=(16, 6))\n",
    "for idex, data in enumerate(train_loader):\n",
    "    if idex<10:\n",
    "        \n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "        \n",
    "        imgs = img.squeeze()\n",
    "        imgs = transform(imgs)\n",
    "        \n",
    "        lspace = linear_model.encoder(img)\n",
    "        lspaces = lspace.squeeze()\n",
    "        image_l = transform(lspaces)\n",
    "        \n",
    "        output = linear_model(img)\n",
    "        output_sq = output.squeeze()\n",
    "        image_o = transform(output_sq)\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(imgs)\n",
    "        plt.title('Original Spectrogram ' + str(idex))\n",
    "        plt.axis('off');\n",
    "        \n",
    "        plt.figure(2)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(image_l)\n",
    "        plt.title('Latent Space ' + str(idex))\n",
    "        plt.axis('off');\n",
    "        \n",
    "        plt.figure(3)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(image_o)\n",
    "        plt.title('Reconstruction ' + str(idex))\n",
    "        plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lspacess = []\n",
    "labels = []\n",
    "transform = transforms.ToPILImage()\n",
    "fig1 = plt.figure(figsize=(16, 6))\n",
    "fig2 = plt.figure(figsize=(16, 6))\n",
    "fig3 = plt.figure(figsize=(16, 6))\n",
    "for idex, data in enumerate(train_loader):\n",
    "    \n",
    "    # plotting\n",
    "    if idex<10:\n",
    "        \n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "\n",
    "        imgs = img.squeeze()\n",
    "        imgs = transform(imgs)\n",
    "\n",
    "        lspace = cnn_model.encoder(img)\n",
    "        lspaces = lspace.squeeze()\n",
    "        image_l = transform(lspaces)\n",
    "\n",
    "        output = cnn_model(img)\n",
    "        output_sq = output.squeeze()\n",
    "        image_o = transform(output_sq)\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(imgs)\n",
    "        plt.title('Original Spectrogram ' + str(idex))\n",
    "        plt.axis('off');\n",
    "\n",
    "        plt.figure(2)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(image_l)\n",
    "        plt.title('Latent Space ' + str(idex))\n",
    "        plt.axis('off');\n",
    "\n",
    "        plt.figure(3)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(image_o)\n",
    "        plt.title('Reconstruction ' + str(idex))\n",
    "        plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_model2 = CNN_autoencoder2().to(device)\n",
    "#num_epochs = 100\n",
    "#optimizer = torch.optim.Adam(conv_model2.parameters(), lr=1e-3)\n",
    "lspaces = []\n",
    "labels = []\n",
    "for data in train_loader:\n",
    "    img, _ = data\n",
    "    img = img.to(device)\n",
    "    lspace = cnn_model.encoder(img)\n",
    "    output = cnn_model(img)\n",
    "    #print(lspace.shape)\n",
    "    lspacef = torch.flatten(lspace, start_dim=1, end_dim=-1)\n",
    "    #print(lspacef.shape)\n",
    "    lspacen = lspacef.to('cpu')\n",
    "    lspacen = lspacen.detach().numpy()\n",
    "    lspaces.append(lspacen)\n",
    "    if data[1][0]=='call':\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited = []\n",
    "for ii in range(0, len(lspaces)):\n",
    "    edited.append(lspaces[ii][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(df)\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=labels, alpha=0.5)\n",
    "plt.legend(['call','no call']) # FIGURE THIS OUT\n",
    "plt.title('UMAP Projection of the Orca Dataset from CNN Output', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_data = pd.DataFrame({'index':np.arange(380),\n",
    "                           'label':labels})\n",
    "hover_data['item'] = hover_data.label.map(\n",
    "    {\n",
    "        0:'no call',\n",
    "        1:'call'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://umap-learn.readthedocs.io/en/0.4dev/plotting.html\n",
    "umap.plot.output_notebook()\n",
    "mapper = umap.UMAP().fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = umap.plot.interactive(mapper, labels=labels, hover_data=hover_data, point_size=8, theme='blue')\n",
    "umap.plot.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, init=\"random\", n_init=10, max_iter=300, random_state=42)\n",
    "label_kmean = kmeans.fit_predict(df)\n",
    "\n",
    "df['kmeanc'] = label_kmean\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=label_kmean, alpha=0.5)\n",
    "plt.title('KMean Cluster Colouring on UMAP Projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToPILImage()\n",
    "fig1 = plt.figure(figsize=(16, 6))\n",
    "fig2 = plt.figure(figsize=(16, 6))\n",
    "for idex, data in enumerate(train_loader):\n",
    "    if idex<10:\n",
    "        \n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "        \n",
    "        imgs = img.squeeze()\n",
    "        imgs = transform(imgs)\n",
    "        \n",
    "        lspace = cnn_model.encoder(img)\n",
    "        lspaces = lspace.squeeze()\n",
    "        image_l = transform(lspaces)\n",
    "        \n",
    "        output = cnn_model(img)\n",
    "        output_sq = output.squeeze()\n",
    "        image_o = transform(output_sq)\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(image_l)\n",
    "        plt.title('kmean label ' + str(label_kmean[idex]))\n",
    "        plt.axis('off');\n",
    "        \n",
    "        plt.figure(2)\n",
    "        plt.subplot(2, 5, idex + 1)\n",
    "        plt.imshow(imgs)\n",
    "        plt.title('kmean label ' + str(label_kmean[idex]))\n",
    "        plt.axis('off');\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "z = tsne.fit_transform(df) \n",
    "\n",
    "plt.scatter(\n",
    "    z[:, 0],\n",
    "    z[:, 1],\n",
    "    c=labels, alpha=0.5)\n",
    "plt.title('T-SNE Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    z[:, 0],\n",
    "    z[:, 1],\n",
    "    c=label_kmean, alpha=0.5)\n",
    "plt.title('TSNE Clustering with K-Mean Cluster Colouring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# stuff that doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake it till u make it bb\n",
    "# https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a\n",
    "def expand_greyscale_image_channels(grey_image):\n",
    "    grey_image = grey_image.to('cpu')\n",
    "    grey_image_arr = np.array(grey_image)\n",
    "    grey_image_arr = np.expand_dims(grey_image_arr, -1)\n",
    "    grey_image_arr_3_channel = grey_image_arr.repeat(3, axis=-1)\n",
    "    grey_image_arr_3_channel = torch.from_numpy(grey_image_arr_3_channel)\n",
    "    grey_image_arr_3_channel = grey_image_arr_3_channel.to(device)\n",
    "    \n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    grey_image_arr_3_channel = torch.squeeze(grey_image_arr_3_channel)\n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    grey_image_arr_3_channel = grey_image_arr_3_channel.T\n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    grey_image_arr_3_channel = grey_image_arr_3_channel.unsqueeze(0)\n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    return grey_image_arr_3_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreT_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = EfficientNet.from_pretrained('efficientnet-b0', include_top=False)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1280, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 224, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(224, 1, kernel_size=4, stride=3, padding=1),\n",
    "            nn.Upsample(1, 1, 2, 2)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = expand_greyscale_image_channels(x)\n",
    "        x = self.encoder.extract_features(x)\n",
    "        #print(x.shape)\n",
    "        lspace = x\n",
    "        for l in self.decoder:\n",
    "            x = l(x)\n",
    "            #print(x.shape)\n",
    "        print(x.shape)\n",
    "        return x, lspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preT_model = PreT_autoencoder().to(device)\n",
    "num_epochs = 100\n",
    "#output, lspace = train_model(conv_model, train_loader, num_epochs)\n",
    "optimizer = torch.optim.Adam(preT_model.parameters(), lr=1e-3)\n",
    "model, output, lspace = train_test_model(preT_model, train_loader, val_loader, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_loader, val_loader, n_epochs, optimizer):\n",
    "    #transform = transforms.ToPILImage()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_batch_losses = []\n",
    "        val_batch_losses = []\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            img    = img.to(device)\n",
    "            output, lspace = model(img)\n",
    "            loss = criterion(output, img.data)\n",
    "            #************************ backward *************************\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_batch_losses.append(loss.item()) \n",
    "            \n",
    "        model = model.eval()\n",
    "        for data in val_loader:\n",
    "            output, lspace = model(img)\n",
    "            loss = criterion(output, img.data)\n",
    "            val_batch_losses.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(train_batch_losses)\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "            \n",
    "        # ***************************** log ***************************\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch [{epoch + 1}/{num_epochs}], Train loss:{train_loss: .4f} Valid:{val_loss: .4f}\")\n",
    "    \n",
    "    ax = plt.figure().gca()\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'])\n",
    "    plt.title('Loss monitoring')\n",
    "    plt.show()\n",
    "    \n",
    "    return model.eval(), output, lspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/thecyphy/train-cnn-model-with-pytorch-21dafb918f48\n",
    "class CNN_autoencoder2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(401408,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,40)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(40, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 1024), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(1024, 401408),\n",
    "            nn.Unflatten(1, torch.Size([128, 56, 56])), \n",
    "            \n",
    "            nn.MaxUnpool2d(2,2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.MaxUnpool2d(2,2), \n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1)\n",
    "        )\n",
    "           \n",
    "    def forward(self, x):\n",
    "        for l in \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model2 = CNN_autoencoder2().to(device)\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(conv_model2.parameters(), lr=1e-3)\n",
    "conv_model = train_test_model2(conv_model2, train_loader, val_loader, num_epochs, optimizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1BAuvW5MgHYiaEFIB7bMPwV0DTr6nFg1X",
     "timestamp": 1680043017434
    },
    {
     "file_id": "1C5dMssJUb_vyXuU4D0NODjattYaroMXR",
     "timestamp": 1680027838253
    },
    {
     "file_id": "1_J2MrBSvsJfOcVmYAN2-WSp36BtsFZCa",
     "timestamp": 1679891532089
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "hotdog",
   "language": "python",
   "name": "hotdog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
