{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project - Orca Call Clustering \n",
    "Karlee Zammit - V00823093 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/earthspecies/library/tree/main/orcas\n",
    "\n",
    "https://debuggercafe.com/pytorch-pretrained-efficientnet-model-image-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: efficientnet_pytorch==0.7.0 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kzammit\\miniconda3\\envs\\hotdog\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image, ImageOps\n",
    "import os.path, sys\n",
    "import tensorflow as tf\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install torch torchvision efficientnet_pytorch==0.7.0\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save the waveform from the wave file \n",
    "def plot_waveform(idx, waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "    axes.grid(True)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "    #plt.savefig(r'C:\\Users\\kzammit\\Repos\\school\\phys555\\project-DL\\orcas_classification\\wavpngs\\wav-' + str(idx) + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save the spectrogram from the wave file\n",
    "#def create_spectrogram(idx, specgram, title=None, ylabel=\"freq_bin\"):\n",
    "def create_spectrogram(idx, specgram):\n",
    "    #fig, axs = plt.subplots(1, 1)\n",
    "    #axs.set_title(title or \"Spectrogram (db)\")\n",
    "    #axs.set_ylabel(ylabel)\n",
    "    #axs.set_xlabel(\"frame\")\n",
    "    #axs.set_ylim(0, 100)\n",
    "    #print(specgram.shape)\n",
    "    #imgf = np.flip(specgram, axis=0)\n",
    "    transform=transforms.ToPILImage()\n",
    "    #specgram=transform(specgram)\n",
    "    specgram= transform(specgram)\n",
    "    specgram = ImageOps.flip(specgram)\n",
    "    #specgram.save(r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\specs\\spec-\" + str(idx) + \".png\")\n",
    "    display(specgram)\n",
    "    \n",
    "    #im = Image.fromarray(librosa.power_to_db(specgram))\n",
    "    #if im.mode != 'RGB':\n",
    "    #    im = im.convert('RGB')\n",
    "    #im.save(\"your_file.jpeg\")\n",
    "    #im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    # librosa... is a numpy array, save that to a png file instead \n",
    "    #fig.colorbar(im, ax=axs)\n",
    "    #plt.show(block=False)\n",
    "    #plt.savefig(r'C:\\Users\\kzammit\\Repos\\school\\phys555\\project-DL\\orcas_classification\\spectrograms\\spec-' + str(idx) + '.png')\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the spectrogram figures and save as pngs\n",
    "def crop(path, dirs):\n",
    "    for item in dirs:\n",
    "        fullpath = os.path.join(path,item)\n",
    "        if os.path.isfile(fullpath):\n",
    "            im = Image.open(fullpath)\n",
    "            f, e = os.path.splitext(fullpath)\n",
    "            # last one does the height\n",
    "            # left top right bottom\n",
    "            imCrop = im.crop((80, 58, 475, 425)) #corrected\n",
    "            #imCrop.save(f + '-crop.png', \"PNG\", quality=300)\n",
    "            display(imCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    transform =  transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], \n",
    "                             std=[0.229]),])\n",
    "    return transform\n",
    "\n",
    "#transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "#                     std=[0.229, 0.224, 0.225]),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Spectrogram Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each available wave file\n",
    "for ii in range(59, 60):\n",
    "    \n",
    "    # In the audio folder \n",
    "    path = r'C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\audio'\n",
    "    \n",
    "    # Get the file name and path\n",
    "    file = '\\\\' + str(ii) + '.wav'\n",
    "    fullfile = path + file\n",
    "\n",
    "    # Set the sample to this file\n",
    "    SAMPLE_WAV = fullfile\n",
    "\n",
    "    # Load the wave with torchaudio \n",
    "    ORCA_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_WAV)\n",
    "\n",
    "    # Plot the original waveform \n",
    "    plot_waveform(idx=str(ii), waveform=ORCA_WAVEFORM, sr=SAMPLE_RATE, title=\"Original waveform\")\n",
    "    Audio(ORCA_WAVEFORM.numpy(), rate=SAMPLE_RATE)\n",
    "\n",
    "    # Set the spectrogram parameters \n",
    "    n_fft = 2000\n",
    "    win_length = 2000\n",
    "    hop_length = 100\n",
    "\n",
    "    # Define spectrogram settings using torch audio transforms  \n",
    "    spectrogram = T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length, center=True, pad_mode=\"reflect\", power=2.0)\n",
    "    \n",
    "    # Calculate the spectrogram \n",
    "    spec = spectrogram(ORCA_WAVEFORM)\n",
    "    \n",
    "    # Plot and save the spectrogram\n",
    "    create_spectrogram(idx=str(ii), specgram=spec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrcaImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, annotations_file, img_dir, transform, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orca_dataset = OrcaImageDataset(annotations_file, img_dir, transform=transforms.Grayscale())\n",
    "#input_height = 367\n",
    "#input_width = 395\n",
    "#perm_val = input_height*input_width\n",
    "#resize = transforms.Resize(size=(input_height, input_width), antialias=True) # aspect ratio is important, and center crop\n",
    "#orca_dataset = OrcaImageDataset(annotations_file, img_dir, transform= transforms.Compose([transforms.Lambda(lambda x: x[:3]), resize, transforms.ToTensor()])) # drop the alpha channel from the image\n",
    "#orca_dataset = OrcaImageDataset(annotations_file, img_dir, transform= transforms.Compose([transforms.Lambda(lambda x: x[:3]), resize, transforms.Grayscale(num_output_channels=1)]))\n",
    "#orca_dataset = OrcaImageDataset(annotations_file, img_dir, transform= transforms.Compose([transforms.Lambda(lambda x: x[:3]), resize]))# drop the alpha channel from the image\n",
    "\n",
    "annotations_file = r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\annotations-edited.csv\"\n",
    "img_dir = r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\specs\"\n",
    "transforms_eff = preprocess()\n",
    "orca_dataset = OrcaImageDataset(annotations_file, img_dir, transform=transforms_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = orca_dataset.img_labels\n",
    "#labels = labels.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_data, test_data, ytrain_labels, test_labels = train_test_split(\n",
    "    orca_dataset, labels, test_size=0.2, random_state=21)\n",
    "\n",
    "#  split a validation set from the training set\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    xtrain_data, ytrain_labels, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples for training is 380\n",
      "The number of samples for validation is 95\n",
      "The number of samples for testing is 119\n"
     ]
    }
   ],
   "source": [
    "print('The number of samples for training is ' + str(len(train_data)))\n",
    "print('The number of samples for validation is ' + str(len(val_data)))\n",
    "print('The number of samples for testing is ' + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the spectrograms and save as images \n",
    "path = r\"C:\\Users\\kzammit\\Documents\\PHYS555\\orcas_classification\\example_crop\"\n",
    "dirs = os.listdir(path)\n",
    "crop(path, dirs)\n",
    "\n",
    "# the initial size of the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orca_dataset.img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of tensor for 60th image in train dataset: ', orca_dataset[60][0].shape)\n",
    "print(orca_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orca_dataset[60][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToPILImage()\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    #print(image.shape)\n",
    "    image = transform(image)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off');\n",
    "    #print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/implementing-under-over-autoencoders-using-pytorch-4ddaf458947e\n",
    "class CNN_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(3,3)),  # from 1 ---> 4, size = 26x26\n",
    "            #nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=(3,3)),  # from 4 --> 16, size = 24x24\n",
    "            #nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(3,3)),  # from 4 --> 16, size = 24x24\n",
    "            #nn.ReLU(),     #\n",
    "            nn.MaxPool2d(2,2),                                              # size=12x12\n",
    "            #nn.Tanh()\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=4, kernel_size=(3,3)),  # 16 --> 4, size=14*14\n",
    "            #nn.ConvTranspose2d(in_channels=1, out_channels=4, kernel_size=(3,3)),  # 16 --> 4, size=14*14\n",
    "            #nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=(2,2), stride=2),  # 4 --> 1, size=28*28\n",
    "            #nn.ReLU(),\n",
    "            nn.Tanh()                  # TanH is recommended as the output image pixel intensities have to lie between -1 & 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            lspace = x\n",
    "        for l in self.decoder:\n",
    "            x = l(x)\n",
    "        return x, lspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_loader, val_loader, n_epochs, optimizer):\n",
    "    #transform = transforms.ToPILImage()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_batch_losses = []\n",
    "        val_batch_losses = []\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            img    = img.to(device)\n",
    "            output, lspace = model(img)\n",
    "            loss = criterion(output, img.data)\n",
    "            #************************ backward *************************\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_batch_losses.append(loss.item()) \n",
    "            \n",
    "        model = model.eval()\n",
    "        for data in val_loader:\n",
    "            output, lspace = model(img)\n",
    "            loss = criterion(output, img.data)\n",
    "            val_batch_losses.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(train_batch_losses)\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "            \n",
    "        # ***************************** log ***************************\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch [{epoch + 1}/{num_epochs}], Train loss:{train_loss: .4f} Valid:{val_loss: .4f}\")\n",
    "    \n",
    "    ax = plt.figure().gca()\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'])\n",
    "    plt.title('Loss monitoring')\n",
    "    plt.show()\n",
    "    \n",
    "    return model.eval(), output, lspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = CNN_autoencoder().to(device)\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(conv_model.parameters(), lr=1e-3)\n",
    "#output, lspace = train_model(conv_model, train_loader, num_epochs)\n",
    "model, output, lspace = train_test_model(conv_model, train_loader, val_loader, num_epochs, optimizier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent space and reconstructed spectrogram\n",
    "transform = transforms.ToPILImage()\n",
    "img = transform(lspace[0][0])\n",
    "display(img)\n",
    "img_o = transform(output[0][0])\n",
    "display(img_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lspace.size())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Net Pretrained Encoder, Convolutional Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake it till u make it bb\n",
    "def expand_greyscale_image_channels(grey_image):\n",
    "    grey_image = grey_image.to('cpu')\n",
    "    grey_image_arr = np.array(grey_image)\n",
    "    grey_image_arr = np.expand_dims(grey_image_arr, -1)\n",
    "    grey_image_arr_3_channel = grey_image_arr.repeat(3, axis=-1)\n",
    "    grey_image_arr_3_channel = torch.from_numpy(grey_image_arr_3_channel)\n",
    "    grey_image_arr_3_channel = grey_image_arr_3_channel.to(device)\n",
    "    \n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    grey_image_arr_3_channel = torch.squeeze(grey_image_arr_3_channel)\n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    grey_image_arr_3_channel = grey_image_arr_3_channel.T\n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    grey_image_arr_3_channel = grey_image_arr_3_channel.unsqueeze(0)\n",
    "    print(grey_image_arr_3_channel.shape)\n",
    "    return grey_image_arr_3_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreT_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = EfficientNet.from_pretrained('efficientnet-b0', include_top=False)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1280, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 224, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(224, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = expand_greyscale_image_channels(x)\n",
    "        x = self.encoder.extract_features(x)\n",
    "        #print(x.shape)\n",
    "        lspace = x\n",
    "        for l in self.decoder:\n",
    "            x = l(x)\n",
    "            #print(x.shape)\n",
    "        print(x.shape)\n",
    "        return x, lspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "torch.Size([1, 1, 224, 224, 3])\n",
      "torch.Size([224, 224, 3])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1, 112, 112])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (112) must match the size of tensor b (224) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#output, lspace = train_model(conv_model, train_loader, num_epochs)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(preT_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model, output, lspace \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreT_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[1;34m(model, train_loader, val_loader, n_epochs, optimizer)\u001b[0m\n\u001b[0;32m     14\u001b[0m img    \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m output, lspace \u001b[38;5;241m=\u001b[39m model(img)\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#************************ backward *************************\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torch\\nn\\functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\hotdog\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (112) must match the size of tensor b (224) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "preT_model = PreT_autoencoder().to(device)\n",
    "num_epochs = 100\n",
    "#output, lspace = train_model(conv_model, train_loader, num_epochs)\n",
    "optimizer = torch.optim.Adam(preT_model.parameters(), lr=1e-3)\n",
    "model, output, lspace = train_test_model(preT_model, train_loader, val_loader, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project minimum: Load a better model, U Map of the latent space, clusters, plot with labels of the call types \n",
    "fix the encoder part with a pretrained network, learn the decoder\n",
    "\n",
    "self.resnet.fc = nn.Identity() # remove the fully connected laye\n",
    "        \n",
    "add a decoder that's not pretrained \n",
    "    \n",
    "try to use a powerful pretrained encoder to a basic one i made (efficient net v2) \n",
    "\n",
    "# my_spe = autoencoder(data)\n",
    "# make list of my spec . features, send that to umap, plot this representation \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, n_epochs):\n",
    "    #transform = transforms.ToPILImage()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(conv_model.parameters(), lr=1e-3)\n",
    "    for epoch in range(n_epochs):\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            img    = img.to(device)\n",
    "            output, lspace = model(img)\n",
    "            loss = criterion(output, img.data)\n",
    "            #************************ backward *************************\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ***************************** log ***************************\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'epoch [{epoch + 1}/{num_epochs}], loss:{loss.item(): .4f}')\n",
    "    return output, lspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b0(pretrained=True)models.efficientnet_b0(pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1BAuvW5MgHYiaEFIB7bMPwV0DTr6nFg1X",
     "timestamp": 1680043017434
    },
    {
     "file_id": "1C5dMssJUb_vyXuU4D0NODjattYaroMXR",
     "timestamp": 1680027838253
    },
    {
     "file_id": "1_J2MrBSvsJfOcVmYAN2-WSp36BtsFZCa",
     "timestamp": 1679891532089
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "hotdog",
   "language": "python",
   "name": "hotdog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
