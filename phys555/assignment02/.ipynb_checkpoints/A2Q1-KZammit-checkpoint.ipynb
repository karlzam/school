{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a3208f-7e35-4f80-9234-82763f1bbf8d",
   "metadata": {},
   "source": [
    "# Phys555 Assignment 2 Question 1\n",
    "Karlee Zammit - V00823093\n",
    "\n",
    "## Datasets, Data Mining, and Data Visualization (using PCA)\n",
    "\n",
    "Q1- Find two 'suitable' and 'challenging' data sets; one for regression and another for a classification problem. Describe the two data sets in one notebook (inputs and targets) and explain how they can be used for regression and classification. Write about works/applications that have been previously done regarding the chosen data sets. Also, conduct data mining, such as feature visualization and present informative data, using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ae256-44aa-4386-ad1a-04dc818fd5df",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Introduction \n",
    "Regression and classification are both predictive problems where past information is used to predict future observations. The goal of regression is to predict numerical values, and the goal of classification is predict categorical values. \n",
    "\n",
    "In this assignment, I present two different datasets - one for classification and one for regression. \n",
    "\n",
    "## Regression\n",
    "As stated in Bishop, C. M. (2006), the goal of regression is to predict the value of one or more continuous target variables t given the value of a D-dimensional vector x of input variables. \n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Below I provide a summary of linear regression as explained in Bishop, C. M. (2006):\n",
    "\n",
    "Linear models form the basis for more sophisticated regression models. Linear models involve a linear combination of input variables\n",
    "\n",
    "$y(x,w) = w_{0} + w_{1}x_{1} + ... + w_{D}x_{D}$\n",
    "\n",
    "where \n",
    "\n",
    "$x = (x_{1},....,x_{D})^{T}$. This is known as linear regression. \n",
    "\n",
    "A significant limitation of linear regression is that it only allows for a linear function of the input variables. \n",
    "\n",
    "For this assignment, the dataset I chose is based on a paper by Insley, S. (2017), which discusses seasonal patterns in ambient noise near Sachs Harbour, Northwest Territories. \n",
    "\n",
    "### K-Nearest Neighbours\n",
    "The KNN regression algorithm has the ability to work well with non-linear relationships, because it uses nearest neighbours to predict future observations. It assumes that similar things exist in close proximity. In KNN, the distance between an existing data point and a query data point are calculated, and the distances are then sorted from smallest to largest. The first K entries are selected, and if a regression problem, the mean of the K labels is returned. \n",
    "\n",
    "## Classification\n",
    "Classification is using one or more variables to predict a categorical variable of interest. I will discuss the basics of classification in the classification section below, using a dataset on mushrooms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057ca2a-3efd-4cd3-8230-91ae7e439e7a",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "I looked at many different datasets before settling on the Sachs Harbor dataset. The Sachs Harbor dataset looks at ambient noise conditions, weather variables such as temperature, dew point, and relative humidity, and ice concentrations at certain radii away from a recorder. The data was used in the paper to perform several regression tasks, which found: \n",
    " \n",
    "- Stronger wind increased noise \n",
    "- Greater ice concentrations decreased noise \n",
    "\n",
    "and stated an area of future work would be to model the impact of increased human activities on ambient noise levels, and predict the impact of these changing levels on marine mammals. \n",
    "\n",
    "*Before I settled on this dataset, I looked at:*\n",
    "\n",
    "Szeged Weather data (analysis included in the appendix): https://www.kaggle.com/datasets/budincsevity/szeged-weather\n",
    "\n",
    "Beans (analysis included in the appendix): http://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset\n",
    "\n",
    "Analysis for the two datasets above is included in the appendix as I did a lot of data mining that took a substantial amount of time. \n",
    "\n",
    "*Others that I chose to delete (reasons in brackets):*\n",
    "\n",
    "CalCOFI Dataset: https://www.kaggle.com/datasets/sohier/calcofi?resource=download (too noisy, not enough samples, not enough complexity, ie. one principal component accounted for 90+% of the data)\n",
    "\n",
    "Abalone: https://archive.ics.uci.edu/ml/datasets/abalone (not enough samples, not enough complexity, ie. one principal component accounted for 90+% of the data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d625a-6b8c-4211-9c91-1d827164be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d21eaf-e716-440a-95ed-b900357ef40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sachs Harbor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21603bc5-9452-458a-af47-2ef1d20d91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data csv and print it's shape\n",
    "df_SH = pd.read_excel('Ambient Sound Data Sachs Harbour 2015-2016.xlsx')\n",
    "print(df_SH.shape)\n",
    "\n",
    "# Count the existing nans\n",
    "nan_count = df_SH.isna().sum().sum()\n",
    "print(nan_count)\n",
    "\n",
    "# Drop non necessary columns \n",
    "df_SH = df_SH.drop(['Deployment', 'Year', 'Month', 'Day', 'Hour', 'DateTime', 'Ice'], axis=1)\n",
    "\n",
    "# Double check there are no nans \n",
    "df_SH=df_SH.dropna(axis=0)\n",
    "\n",
    "# Plot a histogram of the data\n",
    "df_SH.hist(figsize=(14, 12), bins=30, edgecolor=\"black\")\n",
    "print(df_SH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ded43a-091f-436b-838f-33f7293a96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the data\n",
    "\n",
    "# Initialize the normalization estimator \n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input alfalfa data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(df_SH)\n",
    "\n",
    "# Apply the method to the alfalfa data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(df_SH)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Sachs Harbour Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784c9ae-81af-41de-bf5b-e08e2b71884d",
   "metadata": {},
   "source": [
    "Approximately 60% of the information included in the data is contained within the first principal component. 16 principal components are needed to describe 100% of the information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70cf02-3960-4d22-b087-c2b184e87a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_SH.corr()\n",
    "ax = plt.axes()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, ax=ax)\n",
    "plt.title('Correlation Coefficient Matrix for Sachs Harbour Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd22e4-edc9-4924-b564-b95d1ff0ea71",
   "metadata": {},
   "source": [
    "Above is the correlation coefficient matrix for the dataset. Relative humidity, windspeed, and wind direction do not seem to be correlated with temperature, or the ambient frequency levels. Ice concentrations seems to be negatively correlated with the levels within specific ambient sound levels. For the purposes of this assignment, I will show the power of linear regression and KNN, using temperature as the target variable. I will discuss this further in question 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ae5aa-89f7-4e7b-a85c-b85833dd1299",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification Dataset\n",
    "\n",
    "For this assignment, I decided to provide two different datasets for classification as they show different but important things to keep in mind for classification. \n",
    "\n",
    "For classification, I chose the UCI mushroom dataset: https://archive.ics.uci.edu/ml/datasets/mushroom\n",
    "\n",
    "and the Houston Weather dataset: https://www.kaggle.com/datasets/alejandrochapa/houston-weather-data\n",
    "\n",
    "Overall, for classification, the output of an algorithm is a category, a value between 0 and 1 which represents a probability, or a binary value, that represents a predicted value for a given example of input data. This output can be used to understand what label an example input datapoint should be given based on a trained classification algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d07db1-cf70-4ca1-b5f1-4111c4988870",
   "metadata": {},
   "source": [
    "## Mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7b9ba-49c1-4c21-baf2-71101236cb57",
   "metadata": {},
   "source": [
    "Below is data visualization for the mushroom data. For classification, the output for a given input example would be a binary classification of \"edible\" or \"poisonous\". The mushroom dataset has been widely used, and is a popular dataset from the UCI website. There are 10+ papers that cite it, ranging a wide variety of topics such as feature extraction, noise filtering, and categorical clustering. \n",
    "\n",
    "Link to dataset: https://archive.ics.uci.edu/ml/datasets/mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3be83d-4b33-4a65-b9ff-56c7ba16bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and clean \n",
    "df_mushies = pd.read_csv('secondary_data.csv', delimiter=';')\n",
    "print(df_mushies.shape)\n",
    "nan_count = df_mushies.isna().sum().sum()\n",
    "print(nan_count)\n",
    "print(df_mushies.shape)\n",
    "df_mushies = df_mushies.dropna(axis=1, how='any')\n",
    "nan_count = df_mushies.isna().sum().sum()\n",
    "print(nan_count)\n",
    "print(df_mushies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88148da4-0e42-44ee-9dec-f4fc4a8d15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data is categorical \n",
    "df_mushies = df_mushies.astype('category')\n",
    "print(df_mushies)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "for column in df_mushies.columns:\n",
    "    df_mushies[column] = labelencoder.fit_transform(df_mushies[column])\n",
    "\n",
    "df_mushies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27551a2-5613-49cb-aa00-ddd2f5258e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix for the mushroom data \n",
    "corr = df_mushies.corr()\n",
    "ax = plt.axes()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, ax=ax)\n",
    "plt.title('Correlation Coefficient Matrix for Mushroom Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6d1a1-f1dc-45e0-beb0-58475d153797",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Houston Weather (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52da19-5fc7-4470-92a3-fdf9d2f959f5",
   "metadata": {},
   "source": [
    "Below is data visualization for the Houston weather dataset. The classification output would be a binary classification of \"rain\" or \"no rain\" for a given input example. I decided to include this, as it is a very good example of unbalanced data: there are many more non-rainy days than rainy days, and so before this data is used for classification, it will need to be balanced through a exercise such as bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829716df-2531-4bc2-b6af-0169fb0d8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in csv data\n",
    "htx_2006 = pd.read_csv('htx_2006_weather.csv')\n",
    "print(htx_2006.shape)\n",
    "htx_2010 = pd.read_csv('htx_2010_weather.csv')\n",
    "print(htx_2010.shape)\n",
    "htx_2011 = pd.read_csv('htx_2011_weather.csv')\n",
    "print(htx_2011.shape)\n",
    "htx_2012 = pd.read_csv('htx_2012_weather.csv')\n",
    "print(htx_2012.shape)\n",
    "htx_2013 = pd.read_csv('htx_2013_weather.csv')\n",
    "htx_2014 = pd.read_csv('htx_2014_weather.csv')\n",
    "htx_2015 = pd.read_csv('htx_2015_weather.csv')\n",
    "htx_2018 = pd.read_csv('htx_2018_weather.csv')\n",
    "htx_2019 = pd.read_csv('htx_2019_weather.csv')\n",
    "htx_2021 = pd.read_csv('htx_2021_weather.csv')\n",
    "\n",
    "# Append the first two together\n",
    "htx = htx_2006.append(htx_2010)\n",
    "print(htx.shape)\n",
    "\n",
    "# Make a list of the rest of the years \n",
    "years = [htx_2011, htx_2012, htx_2013, htx_2014, htx_2015, htx_2018, htx_2019, htx_2021]\n",
    "\n",
    "# Loop through all of the years and append them \n",
    "for year in years: \n",
    "    htx = htx.append(year)\n",
    "    \n",
    "print(htx.shape)\n",
    "\n",
    "# Drop nans from the data by row\n",
    "htx=htx.dropna(axis=0)\n",
    "print(htx.shape)\n",
    "print(htx.columns)\n",
    "\n",
    "#htx.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "#plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "print(htx)\n",
    "\n",
    "# Determine columns that are not numeric \n",
    "result = htx.applymap(np.isreal)\n",
    "\n",
    "# Replace 'Blank's with nans\n",
    "htx = htx.replace('Blank', np.nan)\n",
    "\n",
    "# And then remove the rows containing these nans\n",
    "htx=htx.dropna(axis=0)\n",
    "print(htx.shape)\n",
    "\n",
    "# Change strings to floats where applicable\n",
    "htx['wind_speed9am'] = pd.to_numeric(htx['wind_speed9am'])\n",
    "htx['wind_speed3pm'] = pd.to_numeric(htx['wind_speed3pm'])\n",
    "htx['humidity9am'] = pd.to_numeric(htx['humidity9am'])\n",
    "htx['humidity3pm'] = pd.to_numeric(htx['humidity3pm'])\n",
    "htx['pressure9am'] = pd.to_numeric(htx['pressure9am'])\n",
    "htx['pressure3pm'] = pd.to_numeric(htx['pressure3pm'])\n",
    "htx['temp9am'] = pd.to_numeric(htx['temp9am'])\n",
    "htx['temp3pm'] = pd.to_numeric(htx['temp3pm'])\n",
    "\n",
    "# Plot a histogram of the data\n",
    "htx.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "htx = htx.drop(['date', 'cloud9am', 'cloud3pm', 'rain_today', 'rain_tomorrow'], axis=1)\n",
    "htx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bb738-6033-473f-8649-4d528b6b2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called \"rain\", and set to 1 if the rainfall\n",
    "# was larger than 0, and 0 if it did not ran. \n",
    "htx.loc[htx['rainfall'] > 0.0, 'rain'] = 1\n",
    "htx.loc[htx['rainfall'] <= 0.0, 'rain'] = 0\n",
    "print(htx)\n",
    "\n",
    "# See how many days it rained on \n",
    "sum(htx['rain'])\n",
    "\n",
    "# Plot a histogram of the rainy vs non-rainy day to see if the data \n",
    "# is balanced\n",
    "htx['rain'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce802b-fd15-4a00-87f7-c474e5cae098",
   "metadata": {},
   "source": [
    "As seen in the above plot, the data is not balanced, meaning there are far fewer rainy days not rainy days. Because of this, when splitting this data into training and testing, it will be thoroughly trained on how to predict not rainy days, but it will perform worse at predicting rainy days. Before classification can be completed, the data should be balanced with a technique such as bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f92f26-1812-46c5-94c9-1334897fadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA analysis\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(htx)\n",
    "\n",
    "# Apply the method to the data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(htx)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Houston Weather Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c39156-1c07-4129-9e30-f0d9f5daaf2c",
   "metadata": {},
   "source": [
    "From the above PCA analysis, 11 principal components are needed to describe 100% of the information in the data. The first principal component can describe approximately 40% of the data, the second approximately 20% of the data, the third less than 20% of the data, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658cdf3f-401b-4ac4-ba65-8dd567c5a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = htx.corr()\n",
    "ax = plt.axes()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, ax=ax)\n",
    "plt.title('Correlation Coefficient Matrix for Houston Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99555fe9-6504-434a-86db-a6a16e574020",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "For this assignment, I will be using the: \n",
    "\n",
    "- Regression: Sachs Harbour Dataset \n",
    "\n",
    "- Classification: Explained using the UCI Mushroom Dataset\n",
    "- Houston Weather Dataset included for classification as an example of unbalanced data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208a64f-0680-46db-9b57-dff9ea540013",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06989f6d-dd35-4006-bf6c-9a149e290f8b",
   "metadata": {},
   "source": [
    "Bishop, C. M. (2006). Pattern recognition and machine learning. In Pattern recognition and machine learning. Springer.\n",
    "\n",
    "**Datasets:**\n",
    "\n",
    "Insley, S. J., Halliday, W. D., & de Jong, T. (2017). Seasonal Patterns in Ocean Ambient Noise near Sachs Harbour, Northwest Territories. Arctic, 70(3), 239â€“248. https://doi.org/10.14430/arctic4662\n",
    "\n",
    "https://www.kaggle.com/datasets/alejandrochapa/houston-weather-data\n",
    "\n",
    "**Appendix Datasets:**\n",
    "https://www.kaggle.com/datasets/budincsevity/szeged-weather\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset\n",
    "\n",
    "**Datasets I Tried but Deleted:**\n",
    "\n",
    "https://calcofi.com/ (too noisy)\n",
    "\n",
    "https://www.kaggle.com/datasets/sohier/calcofi?resource=download (too noisy)\n",
    "\n",
    "Abalone: https://archive.ics.uci.edu/ml/datasets/abalone (not enough samples, not enough complexity, ie. one principal component accounted for 90+% of the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270bffa-a06f-4aac-9052-75dcdbfa8f14",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/Secondary+Mushroom+Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c4899-f2ba-4ddd-af6c-6fc12f6b2c20",
   "metadata": {},
   "source": [
    "# Appendix: Extra Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec2d90-3770-4f5e-be45-98280c13fab0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Szeged Weather (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cc3d1-8ada-43de-9c7f-8456c2d7833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather data csv and print the shape\n",
    "weather = pd.read_csv(r'weatherHistory.csv', delimiter=',')\n",
    "print(weather.shape)\n",
    "\n",
    "# Count nans that exist in the data, and drop rows containing nans\n",
    "nan_count = weather.isna().sum().sum()\n",
    "print(nan_count)\n",
    "weather=weather.dropna(axis=0)\n",
    "\n",
    "# Print the shape of the data after rows containing nans were removed\n",
    "print(weather.shape)\n",
    "print(weather.columns)\n",
    "\n",
    "# Drop non-numerical rows for the purposes of this analysis\n",
    "weather = weather.drop(['Formatted Date', 'Summary', 'Daily Summary', 'Precip Type', 'Loud Cover', 'Visibility (km)'], axis=1)\n",
    "print(weather.columns)\n",
    "print(weather.shape)\n",
    "\n",
    "# Plot histograms of the data\n",
    "weather.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d56d4-f890-4b09-a5cc-ab2c32999652",
   "metadata": {},
   "source": [
    "The pressure and humidity histograms look like they'll need some further data mining. Pressure of 0 millibars is impossible on Earth, and so these values must correspond to instrument error. Humidity of 0 means that there is no water vapour in the air, and it is known that water vapour is always present in the air, even if only in trace amounts. I will remove rows of data corresponding to 0 millibars and rows corresponding to 0 humidity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8461b2-ce50-42e8-9f34-aa4c5190e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with only rows corresponding to zero pressure\n",
    "zero_press_df = weather.loc[ weather['Pressure (millibars)'] == 0]\n",
    "# Print how many rows of 0 pressure there are\n",
    "#print(zero_press_df)\n",
    "print(zero_press_df.shape)\n",
    "\n",
    "# Plot a histogram of 0 pressure data\n",
    "#zero_press_df.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "#plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# Remove 0 pressure from the data, and ensure this was done correctly\n",
    "# by calculating the difference in sizes before and after removal\n",
    "weather_mined = weather.loc[weather['Pressure (millibars)'] != 0]\n",
    "print(weather.shape)\n",
    "print(weather_mined.shape)\n",
    "difference = weather.shape[0] -  weather_mined.shape[0]\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03506a1-7880-4765-9658-aed5a7760af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate zero humidity data\n",
    "zero_humid_df = weather_mined.loc[weather_mined['Humidity'] == 0]\n",
    "print(zero_humid_df.shape)\n",
    "\n",
    "# Plot histograms of this data\n",
    "zero_humid_df.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# As was done with pressure, remove rows corresponding to 0 humidity\n",
    "weather_mined = weather_mined.loc[weather_mined['Humidity'] != 0]\n",
    "print(weather_mined.shape)\n",
    "difference = weather.shape[0] -  weather_mined.shape[0]\n",
    "print(difference) # Diff from original is 1310, which is 1288 + 22 = 1310 so this checks out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260f908-f596-4504-a655-11545ffe2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the mined data once more to see if anything was missed \n",
    "weather_mined.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# This looks a lot nicer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93289b7e-013a-40c6-80db-b02adcdfa301",
   "metadata": {},
   "source": [
    "Although I have performed some data mining, it is still likely that there are outliers, or noise, in this data that I was not able to determine from the rules applied above. This is important to be kept in mind as this noise can affect the prediction of future observations from this known data. This will be discussed further in question 2.\n",
    "\n",
    "I will now perform principal component analysis to show the complexity of the data, and how many components are needed to describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2864db-78be-4381-ad1f-1e6da358aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the data\n",
    "\n",
    "# Initialize the normalization estimator \n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input alfalfa data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(weather_mined)\n",
    "\n",
    "# Apply the method to the alfalfa data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(weather_mined)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Szeged Weather Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d614e-884f-477b-b28d-fb1068765a21",
   "metadata": {},
   "source": [
    "From the above PCA analysis, 5 principal components are needed to describe 100% of the information in the data. The first principal component can describe approximately 43% of the data, the second approximately 38% of the data, the third less than 20% of the data, etc. \n",
    "\n",
    "I concluded that this dataset provided an appropriate level of complexity and number of components for the purposes of this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e47cf5-2c27-4977-a5e5-dd6ab6e5c380",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beans (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6b3de-f713-48b3-a73d-2830fe6fe380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather data csv and print the shape\n",
    "beans = pd.read_excel(r'Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "# Count nans that exist in the data, and drop rows containing nans\n",
    "nan_count = beans.isna().sum().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29423f-fa50-4259-b9f2-c3660d59c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = beans.isna().sum().sum()\n",
    "print(nan_count)\n",
    "\n",
    "beans.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "print(beans.columns)\n",
    "\n",
    "beans = beans.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9f8b5-142b-4e39-8292-93850893c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the data\n",
    "\n",
    "# Initialize the normalization estimator \n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input alfalfa data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(beans)\n",
    "\n",
    "# Apply the method to the alfalfa data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(beans)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Beans Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys555",
   "language": "python",
   "name": "phys555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
