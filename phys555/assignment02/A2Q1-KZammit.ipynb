{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a3208f-7e35-4f80-9234-82763f1bbf8d",
   "metadata": {},
   "source": [
    "# Phys555 Assignment 2 Question 1\n",
    "Karlee Zammit - V00823093\n",
    "\n",
    "## Datasets, Data Mining, and Data Visualization (using PCA)\n",
    "\n",
    "Q1- Find two 'suitable' and 'challenging' data sets; one for regression and another for a classification problem. Describe the two data sets in one notebook (inputs and targets) and explain how they can be used for regression and classification. Write about works/applications that have been previously done regarding the chosen data sets. Also, conduct data mining, such as feature visualization and present informative data, using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ae256-44aa-4386-ad1a-04dc818fd5df",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Introduction \n",
    "Regression and classification are both predictive problems where past information is used to predict future observations. The goal of regression is to predict numerical values, and the goal of classification is predict categorical values. \n",
    "\n",
    "In this assignment, I present two different datasets - one for classification and one for regression. \n",
    "\n",
    "## Regression\n",
    "As stated in Bishop, C. M. (2006), the goal of regression is to predict the value of one or more continuous target variables t given the value of a D-dimensional vector x of input variables. \n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Below I provide a summary of linear regression as explained in Bishop, C. M. (2006):\n",
    "\n",
    "Linear models form the basis for more sophisticated regression models. Linear models involve a linear combination of input variables\n",
    "\n",
    "$y(x,w) = w_{0} + w_{1}x_{1} + ... + w_{D}x_{D}$\n",
    "\n",
    "where \n",
    "\n",
    "$x = (x_{1},....,x_{D})^{T}$. This is known as linear regression. \n",
    "\n",
    "A significant limitation of linear regression is that it only allows for a linear function of the input variables. To extend linear models, linear combinations of fixed non-linear functions can be considered, of the form \n",
    "\n",
    "$y(x,w) = w_{0} + \\sum_{j=1}^{M-1}w_{j}\\sigma_{j}(x)$\n",
    "\n",
    "where $\\sigma_{j}(x)$ are basis functions and the total number of parameters in the model is M. \n",
    "\n",
    "For this assignment, the dataset I chose is based on weather variables. I decided to use the all variables excluding temperature to predict the temperature. \n",
    "\n",
    "### K-Nearest Neighbours\n",
    "The KNN regression algorithm has the ability to work well with non-linear relationships, because it uses nearest neighbours to predict future observations. \n",
    "\n",
    "## Classification\n",
    "Classification is using one or more variables to predict a categorical variable of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057ca2a-3efd-4cd3-8230-91ae7e439e7a",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "There are many available datasets ranging in sample size and complexity available for free online. Although this is true, it was challenging to find a dataset with more than 5000 samples that had more than one principal component corresponding to 90% of the data. Many datasets I explored looked complex at first glance, but after PCA, one principal component could explain the data. This was to my dissapointment as these datasets were well documented, and had many citeable works. As one part of this assignment was to discuss previous work applied to the data, I searched high and low for an appropriate dataset. \n",
    "\n",
    "**I chose the Sachs Harbor dataset to perform my analysis on**, which was provided upon request as referenced in this paper: \n",
    "\n",
    "https://doi.org/10.14430/arctic4662\n",
    "\n",
    "**Before choosing this dataset, I had already done this analysis on two other datasets which I have included in the appendix**. I will discuss the Sachs Harbor dataset, but there are also plots and results available for these two datasets in the appendix:\n",
    "\n",
    "\"Szeged Weather\": https://www.kaggle.com/datasets/budincsevity/szeged-weather\n",
    "\n",
    "\"Beans\": https://archive.ics.uci.edu/ml/datasets/dry+bean+dataset\n",
    "\n",
    "**I decided to include these in my assignment as they required more data mining than the Sachs Harbour dataset**, which took a considerable amount of time. \n",
    "\n",
    "I also performed PCA analysis on these datasets and determined they were insufficient for the purposes of this assignment:\n",
    "\n",
    "Abalone: https://archive.ics.uci.edu/ml/datasets/abalone (not enough samples)\n",
    "\n",
    "CalCOFI Hydrographic Database: https://calcofi.com/index.php?option=com_content&view=category&layout=blog&id=14&Itemid=835 (not enough complexity, ie. one principal component accounted for 90+% of the data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849d625a-6b8c-4211-9c91-1d827164be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d21eaf-e716-440a-95ed-b900357ef40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sachs Harbor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21603bc5-9452-458a-af47-2ef1d20d91d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16696\\3561646860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_SH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_SH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnan_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_SH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnan_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[1;32m-> 1192\u001b[1;33m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m                 )\n\u001b[0;32m   1194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     with get_handle(\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     ) as handle:\n\u001b[0;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'"
     ]
    }
   ],
   "source": [
    "df_SH = pd.read_excel('Ambient Sound Data Sachs Harbour 2015-2016.xlsx')\n",
    "print(df_SH.shape)\n",
    "nan_count = df_SH.isna().sum().sum()\n",
    "print(nan_count)\n",
    "\n",
    "df_SH = df_SH.drop(['Deployment', 'Year', 'Month', 'Day', 'Hour', 'DateTime', 'Ice'], axis=1)\n",
    "\n",
    "#df_SH.hist(figsize=(100, 68), bins=30, edgecolor=\"black\")\n",
    "#plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "df_SH=df_SH.dropna(axis=0)\n",
    "df_SH.hist(figsize=(14, 12), bins=30, edgecolor=\"black\")\n",
    "print(df_SH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ded43a-091f-436b-838f-33f7293a96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the data\n",
    "\n",
    "# Initialize the normalization estimator \n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input alfalfa data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(df_SH)\n",
    "\n",
    "# Apply the method to the alfalfa data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(df_SH)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Sachs Harbour Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70cf02-3960-4d22-b087-c2b184e87a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "553ae5aa-89f7-4e7b-a85c-b85833dd1299",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification Dataset\n",
    "\n",
    "For classification, I decided to stick with the weather theme, and found a dataset that can be used to predict if it will rain or not. This dataset is once again from Kaggle, titled \"Houston Weather Data\".\n",
    "https://www.kaggle.com/datasets/alejandrochapa/houston-weather-data\n",
    "\n",
    "The data is presented in 10 separate csv files, which I append together and perform data mining on so that the dataset could be used for classification. \n",
    "\n",
    "*Once again, I could use Environment Canada's weather data instead for the same classification task. An idea for the future!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829716df-2531-4bc2-b6af-0169fb0d8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in csv data\n",
    "htx_2006 = pd.read_csv('htx_2006_weather.csv')\n",
    "print(htx_2006.shape)\n",
    "htx_2010 = pd.read_csv('htx_2010_weather.csv')\n",
    "print(htx_2010.shape)\n",
    "htx_2011 = pd.read_csv('htx_2011_weather.csv')\n",
    "print(htx_2011.shape)\n",
    "htx_2012 = pd.read_csv('htx_2012_weather.csv')\n",
    "print(htx_2012.shape)\n",
    "htx_2013 = pd.read_csv('htx_2013_weather.csv')\n",
    "htx_2014 = pd.read_csv('htx_2014_weather.csv')\n",
    "htx_2015 = pd.read_csv('htx_2015_weather.csv')\n",
    "htx_2018 = pd.read_csv('htx_2018_weather.csv')\n",
    "htx_2019 = pd.read_csv('htx_2019_weather.csv')\n",
    "htx_2021 = pd.read_csv('htx_2021_weather.csv')\n",
    "\n",
    "# Append the first two together\n",
    "htx = htx_2006.append(htx_2010)\n",
    "print(htx.shape)\n",
    "\n",
    "# Make a list of the rest of the years \n",
    "years = [htx_2011, htx_2012, htx_2013, htx_2014, htx_2015, htx_2018, htx_2019, htx_2021]\n",
    "\n",
    "# Loop through all of the years and append them \n",
    "for year in years: \n",
    "    htx = htx.append(year)\n",
    "    \n",
    "print(htx.shape)\n",
    "\n",
    "# Drop nans from the data by row\n",
    "htx=htx.dropna(axis=0)\n",
    "print(htx.shape)\n",
    "print(htx.columns)\n",
    "\n",
    "#htx.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "#plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "print(htx)\n",
    "\n",
    "# Determine columns that are not numeric \n",
    "result = htx.applymap(np.isreal)\n",
    "\n",
    "# Replace 'Blank's with nans\n",
    "htx = htx.replace('Blank', np.nan)\n",
    "\n",
    "# And then remove the rows containing these nans\n",
    "htx=htx.dropna(axis=0)\n",
    "print(htx.shape)\n",
    "\n",
    "# Change strings to floats where applicable\n",
    "htx['wind_speed9am'] = pd.to_numeric(htx['wind_speed9am'])\n",
    "htx['wind_speed3pm'] = pd.to_numeric(htx['wind_speed3pm'])\n",
    "htx['humidity9am'] = pd.to_numeric(htx['humidity9am'])\n",
    "htx['humidity3pm'] = pd.to_numeric(htx['humidity3pm'])\n",
    "htx['pressure9am'] = pd.to_numeric(htx['pressure9am'])\n",
    "htx['pressure3pm'] = pd.to_numeric(htx['pressure3pm'])\n",
    "htx['temp9am'] = pd.to_numeric(htx['temp9am'])\n",
    "htx['temp3pm'] = pd.to_numeric(htx['temp3pm'])\n",
    "\n",
    "# Plot a histogram of the data\n",
    "htx.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "htx = htx.drop(['date', 'cloud9am', 'cloud3pm', 'rain_today', 'rain_tomorrow'], axis=1)\n",
    "htx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bb738-6033-473f-8649-4d528b6b2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called \"rain\", and set to 1 if the rainfall\n",
    "# was larger than 0, and 0 if it did not ran. \n",
    "htx.loc[htx['rainfall'] > 0.0, 'rain'] = 1\n",
    "htx.loc[htx['rainfall'] <= 0.0, 'rain'] = 0\n",
    "print(htx)\n",
    "\n",
    "# See how many days it rained on \n",
    "sum(htx['rain'])\n",
    "\n",
    "# Plot a histogram of the rainy vs non-rainy day to see if the data \n",
    "# is balanced\n",
    "htx['rain'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6c8eb-1406-4bfd-81f4-a85228d9695b",
   "metadata": {},
   "source": [
    "As seen in the above plot, the data is not balanced, meaning there are far fewer rainy days not rainy days. Because of this, when splitting this data into training and testing, it will be thoroughly trained on how to predict not rainy days, but it will perform worse at predicting rainy days. Before classification can be completed, the data should be balanced with a technique such as bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f92f26-1812-46c5-94c9-1334897fadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA analysis\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(htx)\n",
    "\n",
    "# Apply the method to the data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(htx)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Houston Weather Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9647e-3fd7-4ece-8f50-6b3c67e43826",
   "metadata": {},
   "source": [
    "From the above PCA analysis, 11 principal components are needed to describe 100% of the information in the data. The first principal component can describe approximately 40% of the data, the second approximately 20% of the data, the third less than 20% of the data, etc. \n",
    "\n",
    "### How can this dataset be used for classification? \n",
    "\n",
    "Once data mining and balancing has been completed, this dataset can be used to predict if a future observation will have the outcome of rain or no rain. Classification algorithms use the input variables (in this case min_temp, max_temp, rainfall, wind_speed9am, wind_speed3pm, humidity9am, etc.) to train the machine learning algorithm to understand the connection between these variables, to predict if their specific combination will lead to rain or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208a64f-0680-46db-9b57-dff9ea540013",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06989f6d-dd35-4006-bf6c-9a149e290f8b",
   "metadata": {},
   "source": [
    "Bishop, C. M. (2006). Pattern recognition and machine learning. In Pattern recognition and machine learning. Springer.\n",
    "\n",
    "https://www.kaggle.com/datasets/sohier/calcofi?resource=download\n",
    "\n",
    "https://calcofi.com/\n",
    "\n",
    "https://www.kaggle.com/datasets/budincsevity/szeged-weather\n",
    "\n",
    "https://www.kaggle.com/datasets/alejandrochapa/houston-weather-data\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c4899-f2ba-4ddd-af6c-6fc12f6b2c20",
   "metadata": {},
   "source": [
    "# Appendix: Extra Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec2d90-3770-4f5e-be45-98280c13fab0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Szeged Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cc3d1-8ada-43de-9c7f-8456c2d7833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather data csv and print the shape\n",
    "weather = pd.read_csv(r'weatherHistory.csv', delimiter=',')\n",
    "print(weather.shape)\n",
    "\n",
    "# Count nans that exist in the data, and drop rows containing nans\n",
    "nan_count = weather.isna().sum().sum()\n",
    "print(nan_count)\n",
    "weather=weather.dropna(axis=0)\n",
    "\n",
    "# Print the shape of the data after rows containing nans were removed\n",
    "print(weather.shape)\n",
    "print(weather.columns)\n",
    "\n",
    "# Drop non-numerical rows for the purposes of this analysis\n",
    "weather = weather.drop(['Formatted Date', 'Summary', 'Daily Summary', 'Precip Type', 'Loud Cover', 'Visibility (km)'], axis=1)\n",
    "print(weather.columns)\n",
    "print(weather.shape)\n",
    "\n",
    "# Plot histograms of the data\n",
    "weather.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d56d4-f890-4b09-a5cc-ab2c32999652",
   "metadata": {},
   "source": [
    "The pressure and humidity histograms look like they'll need some further data mining. Pressure of 0 millibars is impossible on Earth, and so these values must correspond to instrument error. Humidity of 0 means that there is no water vapour in the air, and it is known that water vapour is always present in the air, even if only in trace amounts. I will remove rows of data corresponding to 0 millibars and rows corresponding to 0 humidity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8461b2-ce50-42e8-9f34-aa4c5190e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with only rows corresponding to zero pressure\n",
    "zero_press_df = weather.loc[ weather['Pressure (millibars)'] == 0]\n",
    "# Print how many rows of 0 pressure there are\n",
    "#print(zero_press_df)\n",
    "print(zero_press_df.shape)\n",
    "\n",
    "# Plot a histogram of 0 pressure data\n",
    "#zero_press_df.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "#plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# Remove 0 pressure from the data, and ensure this was done correctly\n",
    "# by calculating the difference in sizes before and after removal\n",
    "weather_mined = weather.loc[weather['Pressure (millibars)'] != 0]\n",
    "print(weather.shape)\n",
    "print(weather_mined.shape)\n",
    "difference = weather.shape[0] -  weather_mined.shape[0]\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03506a1-7880-4765-9658-aed5a7760af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate zero humidity data\n",
    "zero_humid_df = weather_mined.loc[weather_mined['Humidity'] == 0]\n",
    "print(zero_humid_df.shape)\n",
    "\n",
    "# Plot histograms of this data\n",
    "zero_humid_df.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# As was done with pressure, remove rows corresponding to 0 humidity\n",
    "weather_mined = weather_mined.loc[weather_mined['Humidity'] != 0]\n",
    "print(weather_mined.shape)\n",
    "difference = weather.shape[0] -  weather_mined.shape[0]\n",
    "print(difference) # Diff from original is 1310, which is 1288 + 22 = 1310 so this checks out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260f908-f596-4504-a655-11545ffe2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the mined data once more to see if anything was missed \n",
    "weather_mined.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# This looks a lot nicer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93289b7e-013a-40c6-80db-b02adcdfa301",
   "metadata": {},
   "source": [
    "Although I have performed some data mining, it is still likely that there are outliers, or noise, in this data that I was not able to determine from the rules applied above. This is important to be kept in mind as this noise can affect the prediction of future observations from this known data. This will be discussed further in question 2.\n",
    "\n",
    "I will now perform principal component analysis to show the complexity of the data, and how many components are needed to describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2864db-78be-4381-ad1f-1e6da358aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the data\n",
    "\n",
    "# Initialize the normalization estimator \n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input alfalfa data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(weather_mined)\n",
    "\n",
    "# Apply the method to the alfalfa data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(weather_mined)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Szeged Weather Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d614e-884f-477b-b28d-fb1068765a21",
   "metadata": {},
   "source": [
    "From the above PCA analysis, 5 principal components are needed to describe 100% of the information in the data. The first principal component can describe approximately 43% of the data, the second approximately 38% of the data, the third less than 20% of the data, etc. \n",
    "\n",
    "I concluded that this dataset provided an appropriate level of complexity and number of components for the purposes of this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ee615-b536-4f42-bb9f-55fb2714ea38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80e47cf5-2c27-4977-a5e5-dd6ab6e5c380",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6b3de-f713-48b3-a73d-2830fe6fe380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather data csv and print the shape\n",
    "beans = pd.read_excel(r'Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "# Count nans that exist in the data, and drop rows containing nans\n",
    "nan_count = beans.isna().sum().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29423f-fa50-4259-b9f2-c3660d59c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = beans.isna().sum().sum()\n",
    "print(nan_count)\n",
    "\n",
    "beans.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "print(beans.columns)\n",
    "\n",
    "beans = beans.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9f8b5-142b-4e39-8292-93850893c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the data\n",
    "\n",
    "# Initialize the normalization estimator \n",
    "sc = StandardScaler()\n",
    "\n",
    "# Train the estimator on the input alfalfa data. This method calculates the mean and variance of each of the features present in the data. \n",
    "sc.fit(beans)\n",
    "\n",
    "# Apply the method to the alfalfa data, to transform all of the features using their respective mean and variance.\n",
    "df_norm = sc.transform(beans)\n",
    "\n",
    "# Initialize scikit learns principal component analysis function\n",
    "pca = PCA()\n",
    "\n",
    "# Determine transformed features\n",
    "df_analysis_pca = pca.fit_transform(df_norm)\n",
    "\n",
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.title('Explained Variance Ratio vs. Principal Component Index for the Beans Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f2e08-2a7d-4c76-95f7-8fcbfacdf573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys555",
   "language": "python",
   "name": "phys555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
