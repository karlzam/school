{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a3208f-7e35-4f80-9234-82763f1bbf8d",
   "metadata": {},
   "source": [
    "# Phys555 Assignment 2 Question 3\n",
    "Karlee Zammit - V00823093\n",
    "\n",
    "# Linear Regression and KNN\n",
    "\n",
    "Q3- Use the Q1 regression data set for the following: \n",
    "Perform a PCA and use the first (N) best components that contain ~90% of the information. Then, repeat Q2 (A and B) with the first N components and compare the performances/plots with Q2. \n",
    "Bonus: Repeat the same steps with 50% and 70% information and compare the three cases (i.e., 50%, 70% and 90%). Add a discussion and conclusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b8ec5e-f722-4f48-9bfd-4acf2cb989ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from pandas.plotting import scatter_matrix    \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import sys\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18903a9e-f12e-4bfc-9a89-9b0fb5c32b27",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b70166-1373-470d-8543-588485fe3d06",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db80ad16-cd54-4c0f-9423-b9a8a35a39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same linear regression model as in Q2 so the results are comparable, for the data keeping only n_components \n",
    "def linear_regression(X_tr, X_va, Y_tr, Y_va, xlim, ylim, textx, texty, text2x, text2y):\n",
    "    reg = linear_model.SGDRegressor(loss='squared_error', penalty='L2', alpha=0.001,\n",
    "                                   max_iter=2000, eta0=.001, tol=0.0001, learning_rate='constant', n_iter_no_change=5)\n",
    "\n",
    "    # Fit the model on the PCA data\n",
    "    reg.fit(X_tr,Y_tr)  # fit the model with training set\n",
    "\n",
    "    #'predictions for training and validation sets'\n",
    "    Y_tr_pred = reg.predict(X_tr)  \n",
    "    Y_va_pred = reg.predict(X_va)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(Y_tr,Y_tr_pred,'ob')\n",
    "    plt.plot(Y_va,Y_va_pred,'.r')\n",
    "\n",
    "    plt.plot(np.arange(xlim,ylim,.1),  np.arange(xlim,ylim,.1),'-k')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Predicted X')\n",
    "    plt.legend(['Training', 'Validation'], loc='best')\n",
    "    # plt.xlim([0,2])\n",
    "    # plt.ylim([0,2])\n",
    "\n",
    "    #Statistical information regarding training and validation predictions\n",
    "    mu_lr = np.mean(Y_tr-Y_tr_pred)\n",
    "    median_lr = np.median(Y_tr-Y_tr_pred)\n",
    "    sigma_lr = np.std(Y_tr-Y_tr_pred)\n",
    "\n",
    "    muv_lr = np.mean(Y_va-Y_va_pred)\n",
    "    medianv_lr = np.median(Y_va-Y_va_pred)\n",
    "    sigmav_lr = np.std(Y_va-Y_va_pred)\n",
    "\n",
    "    textstr = '$\\mu=%.4f$\\n$\\mathrm{med}=%.4f$\\n$\\sigma=%.4f$'%(mu_lr, median_lr, sigma_lr)\n",
    "    textstrv = '$\\mu=%.4f$\\n$\\mathrm{med}=%.4f$\\n$\\sigma=%.4f$'%(muv_lr, medianv_lr, sigmav_lr)\n",
    "\n",
    "    plt.text(textx,texty,textstr, color='b',fontsize=15)\n",
    "    plt.text(text2x,text2y,textstrv, color='r',fontsize=15)\n",
    "    plt.title('Linear Regression')\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.hist(Y_tr_pred-Y_tr,20,color='b',histtype='step',density=True,label='training')\n",
    "    plt.hist(Y_va_pred-Y_va,20,color='r',histtype='step',density=True,label='validation')\n",
    "    plt.xlabel('Predicted - real')\n",
    "    plt.ylabel('Probability (density)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(3)\n",
    "    plt.scatter(Y_tr,Y_tr_pred-Y_tr,label='training',color='b',alpha=.3)\n",
    "    plt.scatter(Y_va,Y_va_pred-Y_va,label='test',color='r',alpha=.3)\n",
    "    plt.xlabel('Y (real)')\n",
    "    plt.ylabel('Y (predicted) - Y (real)')\n",
    "    plt.plot([xlim,ylim],[0,0],'y')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20ac55-f7b8-4486-9e2b-a7af60f54286",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8aab63a-5007-4726-917d-e9b720b6b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(X_tr, X_va, Y_tr, Y_va, xlim, ylim, textx, texty, text2x, text2y):    \n",
    "    KNN = KNeighborsRegressor(n_neighbors=5, weights='uniform')  # set the hyperparameters\n",
    "    KNN.fit(X_tr,Y_tr)  # train it\n",
    "\n",
    "    #'predictions for training and validation sets'\n",
    "    Y_tr_pred= KNN.predict(X_tr)  \n",
    "    Y_va_pred= KNN.predict(X_va)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(Y_tr,Y_tr_pred,'ob')\n",
    "    plt.plot(Y_va,Y_va_pred,'.r')\n",
    "\n",
    "    plt.plot(np.arange(xlim,ylim,.1),  np.arange(xlim,ylim,.1),'-k')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Predicted X')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    #Statistical information regarding training and validation predictions\n",
    "    mu = np.mean(Y_tr-Y_tr_pred)\n",
    "    median = np.median(Y_tr-Y_tr_pred)\n",
    "    sigma = np.std(Y_tr-Y_tr_pred)\n",
    "\n",
    "    muv = np.mean(Y_va-Y_va_pred)\n",
    "    medianv = np.median(Y_va-Y_va_pred)\n",
    "    sigmav = np.std(Y_va-Y_va_pred)\n",
    "\n",
    "    textstr = '$\\mu=%.4f$\\n$\\mathrm{med}=%.4f$\\n$\\sigma=%.4f$'%(mu, median, sigma)\n",
    "    textstrv = '$\\mu=%.4f$\\n$\\mathrm{med}=%.4f$\\n$\\sigma=%.4f$'%(muv, medianv, sigmav)\n",
    "\n",
    "    plt.text(textx,texty,textstr, color='b',fontsize=15)\n",
    "    plt.text(text2x,text2y,textstrv, color='r',fontsize=15)\n",
    "    plt.title('KNN Regression')\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.hist(Y_tr_pred-Y_tr,20,color='b',histtype='step',density=True,label='training')\n",
    "    plt.hist(Y_va_pred-Y_va,20,color='r',histtype='step',density=True,label='validation')\n",
    "    plt.xlabel('Predicted - real')\n",
    "    plt.ylabel('Probability (density)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(3)\n",
    "    plt.scatter(Y_tr,Y_tr_pred-Y_tr,label='training',color='b',alpha=.3)\n",
    "    plt.scatter(Y_va,Y_va_pred-Y_va,label='test',color='r',alpha=.3)\n",
    "    plt.xlabel('Y (real)')\n",
    "    plt.ylabel('Y (predicted) - Y (real)')\n",
    "    plt.plot([xlim,ylim],[0,0],'y')\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee820ba-ea59-4abd-9529-a4c3a1b43677",
   "metadata": {},
   "source": [
    "### Split and Normalize Data, Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0237f93a-3747-470b-80f7-cb060766042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,Y, pca_comps):\n",
    "    # Split the input and target data into test and train, with 75% of the data going to training, and 25% of the data going to testing\n",
    "    X_tr, X_va, Y_tr, Y_va = train_test_split(X,Y.values.ravel(),test_size=0.25)\n",
    "\n",
    "    # Print the shape of the split data\n",
    "    print ('training set == ',np.shape(X_tr),np.shape(Y_tr),',, validation set == ', np.shape(X_va),np.shape(Y_va))\n",
    "    \n",
    "    # Normalize the data, as was done in Q2\n",
    "    scaler_S= StandardScaler().fit(X_tr)  # line #2\n",
    "    X_tr_Norm= scaler_S.transform(X_tr) # line # 3\n",
    "    X_va_Norm= scaler_S.transform(X_va)  # Line #4\n",
    "\n",
    "    # Perform PCA on the normalized data, keeping the number of components corresponding to the decimal percentage in n_components\n",
    "    pca = PCA(n_components=pca_comps)\n",
    "    X_tr_pca = pca.fit_transform(X_tr_Norm)\n",
    "    X_va_pca = pca.fit_transform(X_va_Norm)\n",
    "    \n",
    "    return X_tr_pca, X_va_pca, Y_tr, Y_va"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08667dab-18f1-4d5d-95e2-76177443996c",
   "metadata": {},
   "source": [
    "# Sachs Harbour: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8addd27-7d92-4853-9993-9b97b628292a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15124\\1355552065.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read in the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_SH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_SH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_SH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Deployment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Month'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Day'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Hour'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DateTime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Ice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_SH\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_SH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[1;32m-> 1192\u001b[1;33m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m                 )\n\u001b[0;32m   1194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     with get_handle(\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     ) as handle:\n\u001b[0;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "df_SH = pd.read_excel('Ambient Sound Data Sachs Harbour 2015-2016.xlsx')\n",
    "df_SH = df_SH.drop(['Deployment', 'Year', 'Month', 'Day', 'Hour', 'DateTime', 'Ice'], axis=1)\n",
    "df_SH=df_SH.dropna(axis=0)\n",
    "\n",
    "# Set temperature as the target variable\n",
    "var = ['Temperature']\n",
    "\n",
    "# Drop temperature out of the weather data, and set it as X\n",
    "X = df_SH.loc[:,df_SH.columns.drop(var)]\n",
    "print(X.columns)\n",
    "\n",
    "# Set the target (Y) to be temperature\n",
    "Y = df_SH[var].copy()\n",
    "print(Y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59b5a8-03d9-4b89-a53b-06531d18ad3a",
   "metadata": {},
   "source": [
    "## Comparison of 90% Information vs. All Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31df5b1-5d81-4d2d-88b6-b5d51acf6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f89851-4ec7-4c16-9185-80e5ebdc9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(X_tr_pca, X_va_pca, Y_tr, Y_va, -40, 30, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42705873-a9eb-4d4f-96b4-70143d1adc3a",
   "metadata": {},
   "source": [
    "### Discussion \n",
    "\n",
    "INSERT PICTURE OF Q2 RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235add37-56f3-4cb5-b0ee-ac1108ebfef8",
   "metadata": {},
   "source": [
    "## Comparison of 90%, 70%, and 50% PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913f709-bf22-48a4-8507-3d29fb18736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400b8b5-604d-400d-8b03-9504dcfc9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(X_tr_pca, X_va_pca, Y_tr, Y_va, -40, 30, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d95930-2c13-4074-b498-8af3392bdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2c3c6-aadc-4502-958c-60dfd475b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(X_tr_pca, X_va_pca, Y_tr, Y_va, -40, 30, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75f1f2-8e99-4a5b-83ba-ac8ac3c8940e",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8f885-1c84-4973-8601-65c67e1c88be",
   "metadata": {},
   "source": [
    "# Sachs Harbour: Nearest Neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd5cdb-cacb-4e46-946f-429dd0aafd12",
   "metadata": {},
   "source": [
    "## Comparison of 90% Information vs. All Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59669e02-ce8d-4559-885f-36cfbd1fe616",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60533880-d12e-4fc7-8c15-610a948de3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour(X_tr_pca, X_va_pca, Y_tr, Y_va, -40, 30, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7fa68-dcf0-49dc-978d-95fc46057903",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3058f0-fb1c-4fe9-9346-b137076bd024",
   "metadata": {},
   "source": [
    "## Comparison of 90%, 70%, and 50% PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ca9d5-a162-440e-ab62-375923fef820",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854e98b-e372-4916-ba7c-0a24c1e0e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour(X_tr_pca, X_va_pca, Y_tr, Y_va, -40, 30, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84754ee-5ec3-4720-895f-0e7c1a10f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12b2ab-e608-4463-b7f8-f9cca079efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour(X_tr_pca, X_va_pca, Y_tr, Y_va, -40, 30, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f39cce-b37d-4d27-8f3a-8f48f0cf1d2a",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461cddd-4a1f-47df-9286-1b126a0724e4",
   "metadata": {},
   "source": [
    "# Discussion & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404cef7-9569-480d-9d2d-4be18f261176",
   "metadata": {},
   "source": [
    "# Appendix: Extra Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efa2d9-375a-49f2-a752-fe015c93ecaa",
   "metadata": {},
   "source": [
    "## Szeged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7d35f-1c54-4e3f-b5db-e5341b2fd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and clean it as was done in q1 and q2\n",
    "weather = pd.read_csv(r'weatherHistory.csv', delimiter=',')\n",
    "nan_count = weather.isna().sum().sum()\n",
    "weather=weather.dropna(axis=0)\n",
    "weather = weather.drop(['Formatted Date', 'Summary', 'Daily Summary', 'Precip Type', 'Loud Cover', 'Visibility (km)'], axis=1)\n",
    "weather_mined = weather.loc[ weather['Pressure (millibars)'] != 0]\n",
    "weather_mined = weather_mined.loc[weather_mined['Humidity'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3713f4-f661-4d2f-8cbb-fa17f02999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data, creating a target variable and input variables, and into training and validation sets as was done in question 2\n",
    "var = ['Temperature (C)']\n",
    "\n",
    "X = weather_mined.loc[:, weather_mined.columns.drop(var)]\n",
    "print(X.columns)\n",
    "\n",
    "Y = weather_mined[var].copy()\n",
    "print(Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccabfb-0bd5-4bdb-8fbb-065571dcc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd2e65-bf05-40c9-9c05-29f79dfd120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(X_tr_pca, X_va_pca, Y_tr, Y_va, -20, 40, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af042b23-d8a0-4791-9ec0-3d8869a591d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour(X_tr_pca, X_va_pca, Y_tr, Y_va, -20, 40, 45, 10, 45, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9124e-c493-4bce-a27e-e2854d4b6f9b",
   "metadata": {},
   "source": [
    "## Beans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c91a9-2c48-4fa7-8b58-a743f1035bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans = pd.read_excel(r'Dry_Bean_Dataset.xlsx')\n",
    "beans = beans.drop(['Class'], axis=1)\n",
    "\n",
    "# Set temperature as the target variable\n",
    "var = ['Perimeter']\n",
    "\n",
    "# Drop temperature out of the weather data, and set it as X\n",
    "X = beans.loc[:, beans.columns.drop(var)]\n",
    "print(X.columns)\n",
    "\n",
    "# Set the target (Y) to be temperature\n",
    "Y = beans[var].copy()\n",
    "print(Y.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982422e-2421-4577-9729-2e3c6cfca7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_va_pca, Y_tr, Y_va = split_data(X, Y, pca_comps=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db96ed9-1916-42e1-aad7-544d23592ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(X_tr_pca, X_va_pca, Y_tr, Y_va, 450, 2000, 2100, 800, 2100, 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c649c43-5577-492f-812e-1da656e2d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour(X_tr_pca, X_va_pca, Y_tr, Y_va, 450, 2000, 2100, 800, 2100, 1400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys555",
   "language": "python",
   "name": "phys555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
