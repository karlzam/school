{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a3208f-7e35-4f80-9234-82763f1bbf8d",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Karlee Zammit - V00823093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849d625a-6b8c-4211-9c91-1d827164be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection as md\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e05b0c-fece-4d4b-af17-4bb7c98b26c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f51b5e8-0b58-40ce-842b-fea145364ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(X_tr, X_va, Y_tr, Y_va, n_est, max_d, min_samp, plot):\n",
    "    \n",
    "    reg =RandomForestRegressor(n_estimators=n_est, max_depth=max_d, min_samples_leaf=min_samp, \n",
    "                               bootstrap=True, criterion=\"squared_error\")\n",
    "\n",
    "    ## fitting step\n",
    "    reg.fit(X_tr, Y_tr)\n",
    "\n",
    "    ## predict for any data set. Here training and validation sets\n",
    "    PY_tr =reg.predict(X_tr)\n",
    "    PY_va =reg.predict(X_va)\n",
    "\n",
    "    ## plotting the scatter plots and comparing them (i.e., tr and va sets)\n",
    "    def mu_med_sig(pre,tar):\n",
    "        mu = np.mean(pre-tar)\n",
    "        med = np.median(pre-tar)\n",
    "        std = np.std(pre-tar)\n",
    "        return mu,med,std\n",
    "    \n",
    "    if plot == 1:\n",
    "\n",
    "        fig,axarr = plt.subplots(1,2,figsize=(11,5))\n",
    "\n",
    "        axarr[0].scatter(Y_tr,PY_tr,edgecolor='black',linewidths=2,facecolor=None)\n",
    "        axarr[0].scatter(Y_tr,PY_tr,facecolor='C0',label='Training set')\n",
    "        axarr[0].set_xlabel('Target')\n",
    "        axarr[0].set_ylabel('Predicted')\n",
    "        axarr[0].legend(loc=4)\n",
    "        mu,med,sig = mu_med_sig(PY_tr,Y_tr)\n",
    "        axarr[0].text(0.05,0.95,'$\\mu$: %0.4f \\nmedian: %0.4f \\n$\\sigma$: %0.4f'%(mu,med,sig),\n",
    "                      ha='left',va='top',transform=axarr[0].transAxes)\n",
    "\n",
    "        axarr[1].scatter(Y_va,PY_va,edgecolor='black',linewidths=2,facecolor=None)\n",
    "        axarr[1].scatter(Y_va,PY_va,facecolor='C1',label='Validation set')\n",
    "        axarr[1].set_xlabel('Target')\n",
    "        axarr[1].set_ylabel('Predicted')\n",
    "        axarr[1].legend(loc=4)\n",
    "        mu,med,sig = mu_med_sig(PY_va,Y_va)\n",
    "        axarr[1].text(0.05,0.95,'$\\mu$: %0.4f \\nmedian: %0.4f \\n$\\sigma$: %0.4f'%(mu,med,sig),\n",
    "                      ha='left',va='top',transform=axarr[1].transAxes)\n",
    "        \n",
    "        #for ax in axarr:\n",
    "            #xmin,xmax=-0.05,1.55\n",
    "            #ax.set_xlim(xmin,xmax)\n",
    "            #ax.set_ylim(xmin,xmax)\n",
    "            #ax.plot([xmin,xmax],[xmin,xmax],color='black',ls='dashed',zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a178161-42c4-4707-bed9-9bc0d63cc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_class(X_tr, X_va, Y_tr, Y_va, n_est, max_d, min_samp, plot):\n",
    "    \n",
    "    RFclass= RandomForestClassifier(n_estimators=n_est, criterion='entropy', max_depth=max_d, min_samples_leaf=min_samp, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0., bootstrap=True,\n",
    "                                n_jobs=None, verbose=0)\n",
    "    ## fitting the model:\n",
    "    RFclass.fit(X_tr, Y_tr)\n",
    "\n",
    "    ## predict the response for tr and va sets. We can have two outputs: probability (e.g.,PY_tr_prob ) and  the winner class (e.g.,PY_tr):\n",
    "    PY_tr_prob = RFclass.predict_proba(X_tr)\n",
    "    PY_tr = RFclass.predict(X_tr)\n",
    "\n",
    "    PY_va_prob = RFclass.predict_proba(X_va)\n",
    "    PY_va = RFclass.predict(X_va)\n",
    "\n",
    "    ####---------------------------------------------------------------------- plotting for training set\n",
    "    ####----------------------------------------------------------------------\n",
    "    ####----------------------------------------------------------------------\n",
    "    ## Plot the predicted distributions for the trainig set \n",
    "\n",
    "    if plot == 1:\n",
    "        plt.figure(1)\n",
    "        plt.hist(PY_tr_prob[0][Y_tr[:,0]==0,0],20,color = \"blue\",)\n",
    "        plt.xlim([0,1])\n",
    "        plt.legend(['Detections (TR)'])\n",
    "\n",
    "        plt.figure(2)\n",
    "        plt.hist(PY_tr_prob[0][Y_tr[:,0]==1,0],20,color = \"red\",)\n",
    "        plt.xlim([0,1])\n",
    "        plt.legend(['Non-detections (TR)'])\n",
    "\n",
    "        ## ----------------------------------------------------------------------\n",
    "        ## plot ROC  and estimate the are under the curve for the training set\n",
    "\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        T=dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        for i in range(2):\n",
    "            tpr[i], fpr[i], T[i] = roc_curve(Y_tr[:, i], PY_tr_prob[0][:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(3)\n",
    "\n",
    "        cmap = cm.get_cmap(\"viridis\",50)\n",
    "        plt.scatter(fpr[1], tpr[1], c=T[1],cmap=cmap,vmin=0.,vmax=1)\n",
    "        plt.colorbar()\n",
    "        plt.plot([-0.2,1.2],[-0.2,1.2], '--k')\n",
    "        plt.xlim([-.02,1.02])\n",
    "        plt.ylim([-.02,1.02])\n",
    "\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.title('Area Under the Curve = %.3f' %roc_auc[0])\n",
    "\n",
    "        ####----------------------------------------------------------------------plotting for validation set\n",
    "        ####----------------------------------------------------------------------\n",
    "        ####----------------------------------------------------------------------\n",
    "        ## Plot the prediccted distributions for the validation set \n",
    "\n",
    "        plt.figure(6)\n",
    "        plt.hist(PY_va_prob[0][Y_va[:,0]==0,0],20,color = \"blue\",)\n",
    "        plt.xlim([0,1])\n",
    "        plt.legend(['Detections (VA)'])\n",
    "\n",
    "        plt.figure(7)\n",
    "        plt.hist(PY_va_prob[0][Y_va[:,0]==1,0],20,color = \"red\",)\n",
    "        plt.xlim([0,1])\n",
    "        plt.legend(['Non-detections (VA)'])\n",
    "\n",
    "        ## ----------------------------------------------------------------------\n",
    "        ## plot ROC  and estimate the are under the curve for the validation set\n",
    "\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        T=dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        for i in range(2):\n",
    "            tpr[i], fpr[i], T[i] = roc_curve(Y_va[:, i], PY_va_prob[0][:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(8)\n",
    "        cmap = cm.get_cmap(\"viridis\",50)\n",
    "        plt.scatter(fpr[1], tpr[1], c=T[1],cmap=cmap,vmin=0.,vmax=1)\n",
    "        plt.colorbar()\n",
    "        plt.plot([-0.2,1.2],[-0.2,1.2], '--k')\n",
    "        plt.xlim([-.02,1.02])\n",
    "        plt.ylim([-.02,1.02])\n",
    "\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.title('Area Under the Curve = %.3f' %roc_auc[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090d861f-66f2-4435-aaeb-37a64d2f49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_regression(X,Y):\n",
    "    # Split the input and target data into test and train, with 75% of the data going to training, and 25% of the data going to testing\n",
    "    X_tr, X_va, Y_tr, Y_va = train_test_split(X,Y.values.ravel(),test_size=0.25)\n",
    "\n",
    "    # Print the shape of the split data\n",
    "    print ('training set == ',np.shape(X_tr),np.shape(Y_tr),',, validation set == ', np.shape(X_va),np.shape(Y_va))\n",
    "    \n",
    "    # Normalize the data, as was done in Q2\n",
    "    scaler_S= StandardScaler().fit(X_tr)  # line #2\n",
    "    X_tr_Norm= scaler_S.transform(X_tr) # line # 3\n",
    "    X_va_Norm= scaler_S.transform(X_va)  # Line #4\n",
    "    \n",
    "    n_column = 2\n",
    "\n",
    "    # Plot a figure of the normalized training and validation set to ensure they represent the same distribution and spread\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(X_tr_Norm[:,n_column])\n",
    "    plt.title('Training set')\n",
    "    plt.ylabel('N')\n",
    "    plt.xlabel(\"X\"+str(n_column))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(X_va_Norm[:,n_column])\n",
    "    plt.title('Validation set')\n",
    "    plt.ylabel('N')\n",
    "    plt.xlabel(\"X\"+str(n_column))\n",
    "    \n",
    "    return X_tr_Norm, X_va_Norm, Y_tr, Y_va"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d21eaf-e716-440a-95ed-b900357ef40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sachs Harbour Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21603bc5-9452-458a-af47-2ef1d20d91d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28884\\1782496250.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load in the data csv and print it's shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_SH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_SH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Count the existing nans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[1;32m-> 1192\u001b[1;33m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m                 )\n\u001b[0;32m   1194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     with get_handle(\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     ) as handle:\n\u001b[0;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\phys555\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Ambient Sound Data Sachs Harbour 2015-2016.xlsx'"
     ]
    }
   ],
   "source": [
    "# Load in the data csv and print it's shape\n",
    "df_SH = pd.read_excel('Ambient Sound Data Sachs Harbour 2015-2016.xlsx')\n",
    "print(df_SH.shape)\n",
    "\n",
    "# Count the existing nans\n",
    "nan_count = df_SH.isna().sum().sum()\n",
    "print(nan_count)\n",
    "\n",
    "# Drop non necessary columns \n",
    "df_SH = df_SH.drop(['Deployment', 'Ice', 'DateTime'], axis=1)\n",
    "\n",
    "# Double check there are no nans \n",
    "df_SH=df_SH.dropna(axis=0)\n",
    "\n",
    "# Plot a histogram of the data\n",
    "#df_SH.hist(figsize=(14, 12), bins=30, edgecolor=\"black\")\n",
    "print(df_SH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910870e7-ff5a-4b53-a315-6a3177d4b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set temperature as the target variable\n",
    "var = ['Ice106km2']\n",
    "\n",
    "# Drop temperature out of the weather data, and set it as X\n",
    "X = df_SH.loc[:,df_SH.columns.drop(var)]\n",
    "print(X.columns)\n",
    "\n",
    "# Set the target (Y) to be temperature\n",
    "Y = df_SH[var].copy()\n",
    "print(Y.columns)\n",
    "\n",
    "# Call the split data function\n",
    "X_tr_Norm, X_va_Norm, Y_tr, Y_va = split_data_regression(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c9e9b-90e7-4865-9fdc-fb46cb263daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regression(X_tr_Norm, X_va_Norm, Y_tr, Y_va, n_est = 50, max_d = 100, min_samp = 10, plot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ae5aa-89f7-4e7b-a85c-b85833dd1299",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75604730-8660-4914-af32-1e0fa263a726",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Forest Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23f3ce-d92b-49b8-b561-c6d07f5a8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forest Cover\n",
    "covtype = pd.read_csv('covtype.csv')\n",
    "print(covtype.shape)\n",
    "print(covtype.columns)\n",
    "\n",
    "# \"class\" column as numpy array.\n",
    "y = covtype[\"Cover_Type\"].values\n",
    "print(type(y))\n",
    "\n",
    "# All data except \"class\" column.\n",
    "x = covtype.drop([\"Cover_Type\"], axis=1).values\n",
    "print(x.shape)\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "tar= np.reshape(y,(-1,1))\n",
    "tar = ohe.fit_transform(tar)\n",
    "\n",
    "# Split data for train and test.\n",
    "X_tr, X_va, Y_tr, Y_va = train_test_split(x, tar, test_size=0.25)\n",
    "\n",
    "print ('training set == ',np.shape(X_tr),np.shape(Y_tr),',, validation set == ', np.shape(X_va),np.shape(Y_va))\n",
    "    \n",
    "scaler_S= StandardScaler().fit(X_tr)  # line #2\n",
    "X_tr_Norm= scaler_S.transform(X_tr) # line # 3\n",
    "X_va_Norm= scaler_S.transform(X_va)  # Line #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042dff8-1e54-44ed-912f-41ef2d77b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_class(X_tr, X_va, Y_tr, Y_va, n_est=50, max_d=50, min_samp=10, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf3d6e-bdfa-43f3-a549-f71c7a929442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yellowbrick.classifier \n",
    "import ROCAUC\n",
    "\n",
    "def plot_ROC_curve(model, xtrain, ytrain, xtest, ytest):\n",
    "\n",
    "    # Creating visualization with the readable labels\n",
    "    visualizer = ROCAUC(model, encoder={0: 'functional', \n",
    "                                        1: 'needs repair', \n",
    "                                        2: 'nonfunctional'})\n",
    "                                        \n",
    "    # Fitting to the training data first then scoring with the test data                                    \n",
    "    visualizer.fit(xtrain, ytrain)\n",
    "    visualizer.score(xtest, ytest)\n",
    "    visualizer.show()\n",
    "    \n",
    "    return visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d07db1-cf70-4ca1-b5f1-4111c4988870",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7b9ba-49c1-4c21-baf2-71101236cb57",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/Secondary+Mushroom+Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3be83d-4b33-4a65-b9ff-56c7ba16bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and clean \n",
    "df_mushies = pd.read_csv('secondary_data.csv', delimiter=';')\n",
    "print(df_mushies.shape)\n",
    "nan_count = df_mushies.isna().sum().sum()\n",
    "print(nan_count)\n",
    "print(df_mushies.shape)\n",
    "df_mushies = df_mushies.dropna(axis=1, how='any')\n",
    "nan_count = df_mushies.isna().sum().sum()\n",
    "print(nan_count)\n",
    "print(df_mushies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88148da4-0e42-44ee-9dec-f4fc4a8d15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data is categorical \n",
    "df_mushies = df_mushies.astype('category')\n",
    "print(df_mushies)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "for column in df_mushies.columns:\n",
    "    df_mushies[column] = labelencoder.fit_transform(df_mushies[column])\n",
    "\n",
    "df_mushies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898c04f-7f6a-4830-919e-e19b2c143c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"class\" column as numpy array.\n",
    "y = df_mushies[\"class\"].values\n",
    "print(type(y))\n",
    "\n",
    "# All data except \"class\" column.\n",
    "x = df_mushies.drop([\"class\"], axis=1).values\n",
    "print(x.shape)\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "tar= np.reshape(y,(-1,1))\n",
    "tar = ohe.fit_transform(tar)\n",
    "\n",
    "# Split data for train and test.\n",
    "X_tr, X_va, Y_tr, Y_va = train_test_split(x, tar, test_size=0.25)\n",
    "\n",
    "print ('training set == ',np.shape(X_tr),np.shape(Y_tr),',, validation set == ', np.shape(X_va),np.shape(Y_va))\n",
    "    \n",
    "scaler_S= StandardScaler().fit(X_tr)  # line #2\n",
    "X_tr_Norm= scaler_S.transform(X_tr) # line # 3\n",
    "X_va_Norm= scaler_S.transform(X_va)  # Line #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d857d8-8f7b-4b5b-85fb-dc9988e26913",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_class(X_tr, X_va, Y_tr, Y_va, n_est=100, max_d=25, min_samp=40, plot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6d1a1-f1dc-45e0-beb0-58475d153797",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Houston Weather (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52da19-5fc7-4470-92a3-fdf9d2f959f5",
   "metadata": {},
   "source": [
    "Below is data visualization for the Houston weather dataset. The classification output would be a binary classification of \"rain\" or \"no rain\" for a given input example. I decided to include this, as it is a very good example of unbalanced data: there are many more non-rainy days than rainy days, and so before this data is used for classification, it will need to be balanced through a exercise such as bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829716df-2531-4bc2-b6af-0169fb0d8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in csv data\n",
    "htx_2006 = pd.read_csv('htx_2006_weather.csv')\n",
    "print(htx_2006.shape)\n",
    "htx_2010 = pd.read_csv('htx_2010_weather.csv')\n",
    "print(htx_2010.shape)\n",
    "htx_2011 = pd.read_csv('htx_2011_weather.csv')\n",
    "print(htx_2011.shape)\n",
    "htx_2012 = pd.read_csv('htx_2012_weather.csv')\n",
    "print(htx_2012.shape)\n",
    "htx_2013 = pd.read_csv('htx_2013_weather.csv')\n",
    "htx_2014 = pd.read_csv('htx_2014_weather.csv')\n",
    "htx_2015 = pd.read_csv('htx_2015_weather.csv')\n",
    "htx_2018 = pd.read_csv('htx_2018_weather.csv')\n",
    "htx_2019 = pd.read_csv('htx_2019_weather.csv')\n",
    "htx_2021 = pd.read_csv('htx_2021_weather.csv')\n",
    "\n",
    "# Append the first two together\n",
    "htx = htx_2006.append(htx_2010)\n",
    "print(htx.shape)\n",
    "\n",
    "# Make a list of the rest of the years \n",
    "years = [htx_2011, htx_2012, htx_2013, htx_2014, htx_2015, htx_2018, htx_2019, htx_2021]\n",
    "\n",
    "# Loop through all of the years and append them \n",
    "for year in years: \n",
    "    htx = htx.append(year)\n",
    "    \n",
    "print(htx.shape)\n",
    "\n",
    "# Drop nans from the data by row\n",
    "htx=htx.dropna(axis=0)\n",
    "print(htx.shape)\n",
    "print(htx.columns)\n",
    "\n",
    "#htx.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "#plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "print(htx)\n",
    "\n",
    "# Determine columns that are not numeric \n",
    "result = htx.applymap(np.isreal)\n",
    "\n",
    "# Replace 'Blank's with nans\n",
    "htx = htx.replace('Blank', np.nan)\n",
    "\n",
    "# And then remove the rows containing these nans\n",
    "htx=htx.dropna(axis=0)\n",
    "print(htx.shape)\n",
    "\n",
    "# Change strings to floats where applicable\n",
    "htx['wind_speed9am'] = pd.to_numeric(htx['wind_speed9am'])\n",
    "htx['wind_speed3pm'] = pd.to_numeric(htx['wind_speed3pm'])\n",
    "htx['humidity9am'] = pd.to_numeric(htx['humidity9am'])\n",
    "htx['humidity3pm'] = pd.to_numeric(htx['humidity3pm'])\n",
    "htx['pressure9am'] = pd.to_numeric(htx['pressure9am'])\n",
    "htx['pressure3pm'] = pd.to_numeric(htx['pressure3pm'])\n",
    "htx['temp9am'] = pd.to_numeric(htx['temp9am'])\n",
    "htx['temp3pm'] = pd.to_numeric(htx['temp3pm'])\n",
    "\n",
    "# Plot a histogram of the data\n",
    "htx.hist(figsize=(10, 8), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "htx = htx.drop(['date', 'cloud9am', 'cloud3pm', 'rain_today', 'rain_tomorrow'], axis=1)\n",
    "htx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bb738-6033-473f-8649-4d528b6b2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called \"rain\", and set to 1 if the rainfall\n",
    "# was larger than 0, and 0 if it did not ran. \n",
    "htx.loc[htx['rainfall'] > 0.0, 'rain'] = 1\n",
    "htx.loc[htx['rainfall'] <= 0.0, 'rain'] = 0\n",
    "print(htx)\n",
    "\n",
    "# See how many days it rained on \n",
    "sum(htx['rain'])\n",
    "\n",
    "# Plot a histogram of the rainy vs non-rainy day to see if the data \n",
    "# is balanced\n",
    "htx['rain'].hist()\n",
    "\n",
    "# \"class\" column as numpy array.\n",
    "y = htx[\"rain\"].values\n",
    "print(type(y))\n",
    "\n",
    "# All data except \"class\" column.\n",
    "x = htx.drop([\"rain\"], axis=1).values\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3b9c1-715a-4235-9952-19ed481b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "tar= np.reshape(y,(-1,1))\n",
    "tar = ohe.fit_transform(tar)\n",
    "\n",
    "# Split data for train and test.\n",
    "X_tr, X_va, Y_tr, Y_va = train_test_split(x, tar, test_size=0.25)\n",
    "\n",
    "print ('training set == ',np.shape(X_tr),np.shape(Y_tr),',, validation set == ', np.shape(X_va),np.shape(Y_va))\n",
    "    \n",
    "scaler_S= StandardScaler().fit(X_tr)  # line #2\n",
    "X_tr_Norm= scaler_S.transform(X_tr) # line # 3\n",
    "X_va_Norm= scaler_S.transform(X_va)  # Line #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363b4bb-9906-4e3e-916c-c34140fe203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_class(X_tr, X_va, Y_tr, Y_va, n_est=50, max_d=5, min_samp=10, plot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208a64f-0680-46db-9b57-dff9ea540013",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06989f6d-dd35-4006-bf6c-9a149e290f8b",
   "metadata": {},
   "source": [
    "Bishop, C. M. (2006). Pattern recognition and machine learning. In Pattern recognition and machine learning. Springer.\n",
    "\n",
    "**Datasets:**\n",
    "\n",
    "Insley, S. J., Halliday, W. D., & de Jong, T. (2017). Seasonal Patterns in Ocean Ambient Noise near Sachs Harbour, Northwest Territories. Arctic, 70(3), 239â€“248. https://doi.org/10.14430/arctic4662\n",
    "\n",
    "https://www.kaggle.com/datasets/alejandrochapa/houston-weather-data\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Secondary+Mushroom+Dataset\n",
    "\n",
    "https://bg.copernicus.org/articles/10/5793/2013/ (Weather one I haven't tried) \n",
    "\n",
    "https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys555",
   "language": "python",
   "name": "phys555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
