{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cc9c6a-c9ec-46b6-9200-123c5afaf9ae",
   "metadata": {},
   "source": [
    "# PHYS555 - Machine Learning Project - Q1\n",
    "Karlee Zammit - V00823093"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f881c2-a501-4377-a68b-7ffd2abcd7c5",
   "metadata": {},
   "source": [
    "## Describe how SVM algorithms can be used for classification and regression problems (describe the algorithms). Which parameters are the most important ones in the models for classification and regression (e.g., for fitting and controlling overfitting...)? What is the difference between classification and regression algorithms in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b26d9a-14f8-4ea9-9a9d-1f39b719212d",
   "metadata": {},
   "source": [
    "# Support Vector Machine \n",
    "\n",
    "A support vector machine (SVM) is a supervised learning model that analyzes data for classification and regression tasks. At a high level, the SVM algorithm maximizes a particular mathetical function with respect to a given collection of data. As discussed in Noble, W. (2006), there are four concepts essential in the SVM algorithm: \n",
    "\n",
    "1. The separating hyperplane\n",
    "\n",
    "2. The maximum-margin hyperplane\n",
    "\n",
    "3. The soft margin \n",
    "\n",
    "4. The kernel function\n",
    "\n",
    "I will discuss these below for a binary classification example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a5aa1-90ee-4c35-9837-eae6d72a85a3",
   "metadata": {},
   "source": [
    "### The Separating Hyperplane\n",
    "\n",
    "\n",
    "For an imaginary dataset, that looks like the left panel of the Figure below (titled \"2D Hyperplane\"), a separating line can be drawn through the data. Then for a future prediction, depending on where the prediction falls on the graph, a classification can be made if it will belong to the purple or orange class. This separating line is called the separating hyperplane. This idea can be extended to higher dimensions, with a 3-dimensional example provided in the right panel of the Figure below (titled \"3D Hyperplane).\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-hyperplanes.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e03768-ec30-463e-8d14-c818912f1655",
   "metadata": {},
   "source": [
    "### The Maximum-Margin Hyperplane\n",
    "\n",
    "In a 1D example as shown in the Figure below, the \"maximum-margin\" hyperplane is located at the position in space that maximizes it's distance from each of the two classes. If you were to move this margin closer to one class, it would no longer be the maximum distance away and therefore would have a higher chance of inaccurately predicting a future observation of each class. For perfect data like in the Figure below, the maximum-margin hyperplane can be used to determine the optimized hyperplane location.\n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-margin-max.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba671a8d-b64b-4d8a-8191-d71cadca15b4",
   "metadata": {},
   "source": [
    "### The Soft Margin Hyperplane\n",
    "\n",
    "But what if the data  was not perfect, as shown in the Figure below? It would then be ideal to allow for misclassifications, so that future observations can be more accurately predicted (ie. avoid overfitting to the data). This is an example of the tradeoff between bias and variance, which is a common theme in machine learning algorithms. The location of this soft margin is determined by trial and error using cross validation. \n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-margin-soft.png\" width=\"350\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030d700-343e-4c2c-ae3e-c28d1e2a6fda",
   "metadata": {},
   "source": [
    "### The Kernel Function\n",
    "\n",
    "Sometimes data is too complex to be overcome by the introduction of a soft margin alone. For example, in the top panel of the Figure below, there exists no linear line that could separate the two classes from one another. The kernel function provides a solution to the problem, adding an additional dimension to the data. In this example, by squaring the original values, a new dimension is introduced and a linear line can then be used to separate the classes from one another. It can be proven that for any given labelled data set, there exists a kernel function that allows the data to be linearly separated. One needs to consider the curse of dimensionality here, as complex data can be projected into higher and higher dimensions, but the number of possible solutions increases exponentially. \n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-kernel.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe53f8-bf21-4c82-aebc-e5c3ecf90eb8",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting\n",
    "\n",
    "The two goals of SVM are: \n",
    "- Increase the distance of decision boundary to classes (or support vectors)\n",
    "- Maximize the number of points that are correctly classified in the training set\n",
    "\n",
    "For SVM, the most important parameters for avoiding overfitting are \"C\" and \"Gamma\". \n",
    "\n",
    "## C \n",
    "\n",
    "C adds a penalty for each misclassified data point, meaning it tells the SVM optimization how much you want to avoid misclassifying each training example. If C is small, there is a small penalty for misclassified points, and so the decision boundary with a large margin is chosen at the expense of many misclassified points. If C is large, SVM tries to minimize the number of misclassified examples, which results in a decision boundary with a smaller margin. \n",
    "\n",
    "## Gamma\n",
    "\n",
    "Gamma controls the distance of the influence of a single training point. Low values of gamma result in a large similarity radius, and so more points are grouped together. High values of gamma mean that more points need to be grouped together in order to be considered in the same group or class. Large gamma values tend to lead to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e450daa-5f83-401c-ad48-f25347a15381",
   "metadata": {},
   "source": [
    "# Difference between Classification and Regression Algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43422610-6bc1-414a-b89d-a4bc7b5bf795",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "Bishop, C. M. (2006). Pattern recognition and machine learning. In Pattern recognition and machine learning. Springer.\n",
    "\n",
    "Noble, W. (2006). What is a support vector machine?. Nat Biotechnol 24, 1565â€“1567 (2006). https://doi.org/10.1038/nbt1206-1565\n",
    "\n",
    "https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "Scikit-Learn Documentation: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html#svm-regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0aa7d-548e-48c9-926c-161c7a20777c",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f1948-00a7-44bd-b527-119a0586369e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sklearn Function Documentation Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ef37e-ffe9-498f-8d58-ee1721375e18",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]\n",
    "\n",
    "1. C: regularization parameter\n",
    "2. kernel: kernel type to be used in the algorithm, rbf is default\n",
    "3. degree: only used in poly kernel, degree of polynomial\n",
    "4. gamma: scale or auto, coefficient for rbf, poly, or sigmoid \n",
    "5. coef0: only used in poly or sigmoid, independent term in kernel function\n",
    "6. shrinking: whether to use shrinking heuristic. default true\n",
    "7. probability: enable probability estimates, much slower\n",
    "8. tol: tolerance for stopping criterion\n",
    "9. cache_size: kernel cache size\n",
    "10. class_weight: can use this to balance unbalanced data\n",
    "11. verbose: enable verbose output\n",
    "12. max_iter: -1 for no limit\n",
    "13. decision_function_shape: ovo or ovr, ovr is constructed from ovo output\n",
    "14. break_ties: uses lots of computational resources \n",
    "15. random_state: if probability is true, controls random number generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05177bcc-5d02-4f8f-b060-d6c1fa524ee3",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)[source]\n",
    "\n",
    "Not listing each description again, as same as above. Note differences, where these parameters are specific to classification. \n",
    "\n",
    "1. No probability \n",
    "2. No class weight \n",
    "3. No decision function shape\n",
    "4. No break ties \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ea41f-382c-4607-96f1-5f12e27cdf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys555",
   "language": "python",
   "name": "phys555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
