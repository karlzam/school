{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cc9c6a-c9ec-46b6-9200-123c5afaf9ae",
   "metadata": {},
   "source": [
    "# PHYS555 - Machine Learning Project - Q1\n",
    "\n",
    "**Describe how SVM algorithms can be used for classification and regression problems (describe the algorithms). Which parameters are the most important ones in the models for classification and regression (e.g., for fitting and controlling overfitting...)? What is the difference between classification and regression algorithms in SVM?**\n",
    "\n",
    "Karlee Zammit - V00823093"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f881c2-a501-4377-a68b-7ffd2abcd7c5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I will discuss what a support vector machine (SVM) is, how the algorithm works, parameters to avoid overfitting, and the difference between the classification and regression SVM algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b26d9a-14f8-4ea9-9a9d-1f39b719212d",
   "metadata": {},
   "source": [
    "# Support Vector Machine \n",
    "\n",
    "A SVM is a supervised learning model (or decision machine) that analyzes data for classification and regression tasks. At a high level, the SVM algorithm maximizes a particular mathematical function with respect to a given collection of data. As discussed in Bishop, C. M. (2006)., an important property of SVM is that any local solution is also a global optimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a5aa1-90ee-4c35-9837-eae6d72a85a3",
   "metadata": {},
   "source": [
    "Before diving into the math behind the algorithm and the algorithm itself, I present four key concepts. As discussed in Noble, W. (2006), there are four concepts essential in the SVM algorithm: \n",
    "\n",
    "1. The separating hyperplane\n",
    "\n",
    "2. The maximum-margin hyperplane\n",
    "\n",
    "3. The soft margin \n",
    "\n",
    "4. The kernel function\n",
    "\n",
    "I will discuss these below for a binary classification example with linear SVM. Multiclass classification can be performed if a \"one vs rest\" approach is taken with the data.\n",
    "\n",
    "### The Separating Hyperplane\n",
    "\n",
    "\n",
    "For an imaginary dataset, that looks like the left panel of the Figure below (titled \"2D Hyperplane\"), a separating line can be drawn through the data. Then for a future prediction, depending on where the prediction falls on the graph, a classification can be made if it will belong to the purple or orange class. This separating line is called the separating hyperplane. This idea can be extended to higher dimensions, with a 3-dimensional example provided in the right panel of the Figure below (titled \"3D Hyperplane).\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-hyperplanes.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e03768-ec30-463e-8d14-c818912f1655",
   "metadata": {},
   "source": [
    "### The Maximum-Margin Hyperplane\n",
    "\n",
    "In a 1D example as shown in the Figure below, the \"maximum-margin\" hyperplane is located at the position in space that maximizes it's distance from each of the two classes. If you were to move this margin closer to one class, it would no longer be the maximum distance away and therefore would have a higher chance of inaccurately predicting a future observation of each class. For perfect data like in the Figure below, the maximum-margin hyperplane can be used to determine the optimized hyperplane location.\n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-margin-max.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba671a8d-b64b-4d8a-8191-d71cadca15b4",
   "metadata": {},
   "source": [
    "But what if the data was not easily linearly separatable, as shown in the Figure below? It would then be ideal to allow for misclassifications, so that future observations can be more accurately predicted (ie. avoid overfitting to the data). This is an example of the tradeoff between bias and variance, which is a common theme in machine learning algorithms. The location of this soft margin is determined by trial and error using cross validation. \n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-margin-soft.png\" width=\"350\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030d700-343e-4c2c-ae3e-c28d1e2a6fda",
   "metadata": {},
   "source": [
    "### The Kernel Function\n",
    "\n",
    "Sometimes data is too complex to be overcome by the introduction of a soft margin alone. For example, in the top panel of the Figure below, there exists no linear line that could separate the two classes from one another. The kernel function provides a solution to the problem, adding an additional dimension to the data. In this example, by squaring the original values, a new dimension is introduced and a linear line can then be used to separate the classes from one another. It can be proven that for any given labelled data set, there exists a kernel function that allows the data to be linearly separated. One needs to consider the curse of dimensionality here, as complex data can be projected into higher and higher dimensions, but the number of possible solutions increases exponentially. \n",
    "\n",
    "<div>\n",
    "<img src=\"SVM-kernel.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "### The Algorithm \n",
    "\n",
    "For any data set, there may be multiple hyperplanes that exist that can separate the data. Support vectors are data points that are close to the hyperplane and influence the position and orientation of the hyperplane. A variety of parameters are important in fine-tuning the optimized hyperplane, and these are discussed below in the \"Avoiding Overfitting\" section.\n",
    "\n",
    "For classification, SVM finds the optimal hyperplane solution by maximizing the distance between the two classes (allowing for a soft boundary when necessary). \n",
    "\n",
    "For regression, SVM finds the optimal hyperplane, allowing for data points at a distance epsilon away from the hyperplane to be included with no increased error. \n",
    "\n",
    "<div>\n",
    "<img src=\"SVM.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe53f8-bf21-4c82-aebc-e5c3ecf90eb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Avoiding Overfitting\n",
    "\n",
    "For SVC, the most important parameters for avoiding overfitting are \"C\" and \"Gamma\". For SVR, the most important parameters are \"C\", \"Gamma\", and \"Epsilon\". \n",
    "\n",
    "### C (Regularization Parameter)\n",
    "\n",
    "C adds a penalty for each misclassified data point, meaning it tells the SVM optimization how much you want to avoid misclassifying each training example. If C is small, there is a small penalty for misclassified points, and so the decision boundary with a large margin is chosen at the expense of many misclassified points. If C is large, SVM tries to minimize the number of misclassified examples, which results in a decision boundary with a smaller margin. If C is too large, this can cause overfitting.\n",
    "\n",
    "<div>\n",
    "<img src=\"C-corr.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### Gamma (Kernel Coefficient) \n",
    "\n",
    "Gamma controls the distance of the influence of a single training point. Low values of gamma result in a large similarity radius, and so more points are grouped together. High values of gamma mean that less points need to be grouped together in order to be considered in the same group or class. Large gamma values tend to lead to overfitting. \n",
    "\n",
    "<div>\n",
    "<img src=\"Untitled_Artwork (6).png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Epsilon (for Regression Only) \n",
    "\n",
    "The below figure from Chapter 7 of Bishop, C. M. (2006). provides an excellent summary of epsilon.\n",
    "\n",
    "<div>\n",
    "<img src=\"fig77.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The error for points within the epsilon away from the optimal hyperplane is disregarded. Another name for this is called the \"epsilon insensitive tube\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08956b66-f11d-4f1b-946b-8e52e23a411b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Math Behind the Algorithm (Bishop, C. M. (2006) Summary)\n",
    "\n",
    "Chapter 7 of Bishop, C. M. (2006) provides an excellent explanation of the SVM algorithm, of which I provide a brief summary below.\n",
    "\n",
    "For a linear function of the form \n",
    "<div>\n",
    "<img src=\"gen-form.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "where w are the polynomial coefficients, φ is a fixed feature-space transformation, and b is an explicit bias.\n",
    "\n",
    "SVM solves the optimization problem: \n",
    "\n",
    "<div>\n",
    "<img src=\"opt.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "through the use of Lagrange multipliers. \n",
    "\n",
    "We introduce a slack variable ξ, which allows data points to be on the \"wrong side\" of the margin boundary, with a penalty that increases with distance from that boundary. \n",
    "\n",
    "For **classification**, to maximize the margin while applying a penalty to points that lie on the wrong side of the margin, we minimize \n",
    "\n",
    "<div>\n",
    "<img src=\"class-min.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "where C>0, and controls the trade-off between the slack variable penalty and the margin. \n",
    "\n",
    "For **regression**, using Langrage multipliers, the error function \n",
    "\n",
    "<div>\n",
    "<img src=\"err-regression.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "is minimized, where ξ-hat is introduced by the epsilon-tube described in the above section.\n",
    "\n",
    "\n",
    "## Summary: \n",
    "\n",
    "The two goals of SVM for classification are: \n",
    "- Increase the distance of decision boundary to classes (or support vectors)\n",
    "- Maximize the number of points that are correctly classified in the training set\n",
    "\n",
    "The goal of SVM for regression is to determine an optimal hyperplane and epsilon-tube containing the maximum number of points from the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43422610-6bc1-414a-b89d-a4bc7b5bf795",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "Bishop, C. M. (2006). Pattern recognition and machine learning. In Pattern recognition and machine learning. Springer.\n",
    "\n",
    "Noble, W. (2006). What is a support vector machine?. Nat Biotechnol 24, 1565–1567 (2006). https://doi.org/10.1038/nbt1206-1565\n",
    "\n",
    "Suthaharan, S. (2016). Support Vector Machine. In: Machine Learning Models and Algorithms for Big Data Classification. Integrated Series in Information Systems, vol 36. Springer, Boston, MA. https://doi-org.ezproxy.library.uvic.ca/10.1007/978-1-4899-7641-3_9\n",
    "\n",
    "https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "Scikit-Learn Documentation: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html#svm-regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0aa7d-548e-48c9-926c-161c7a20777c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f1948-00a7-44bd-b527-119a0586369e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sklearn Function Documentation Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ef37e-ffe9-498f-8d58-ee1721375e18",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]\n",
    "\n",
    "1. C: regularization parameter\n",
    "2. kernel: kernel type to be used in the algorithm, rbf is default\n",
    "3. degree: only used in poly kernel, degree of polynomial\n",
    "4. gamma: scale or auto, coefficient for rbf, poly, or sigmoid \n",
    "5. coef0: only used in poly or sigmoid, independent term in kernel function\n",
    "6. shrinking: whether to use shrinking heuristic. default true\n",
    "7. probability: enable probability estimates, much slower\n",
    "8. tol: tolerance for stopping criterion\n",
    "9. cache_size: kernel cache size\n",
    "10. class_weight: can use this to balance unbalanced data\n",
    "11. verbose: enable verbose output\n",
    "12. max_iter: -1 for no limit\n",
    "13. decision_function_shape: ovo or ovr, ovr is constructed from ovo output\n",
    "14. break_ties: uses lots of computational resources \n",
    "15. random_state: if probability is true, controls random number generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05177bcc-5d02-4f8f-b060-d6c1fa524ee3",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)[source]\n",
    "\n",
    "Not listing each description again, as same as above. Note differences, where these parameters are specific to classification. \n",
    "\n",
    "1. No probability \n",
    "2. No class weight \n",
    "3. No decision function shape\n",
    "4. No break ties \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ea41f-382c-4607-96f1-5f12e27cdf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys555",
   "language": "python",
   "name": "phys555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
